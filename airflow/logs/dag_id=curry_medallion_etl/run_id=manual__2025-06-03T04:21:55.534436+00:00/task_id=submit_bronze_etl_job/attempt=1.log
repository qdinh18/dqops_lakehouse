[2025-06-03T04:21:56.996+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-06-03T04:21:57.007+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: curry_medallion_etl.submit_bronze_etl_job manual__2025-06-03T04:21:55.534436+00:00 [queued]>
[2025-06-03T04:21:57.012+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: curry_medallion_etl.submit_bronze_etl_job manual__2025-06-03T04:21:55.534436+00:00 [queued]>
[2025-06-03T04:21:57.013+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 1
[2025-06-03T04:21:57.019+0000] {taskinstance.py:2889} INFO - Executing <Task(SparkSubmitOperator): submit_bronze_etl_job> on 2025-06-03 04:21:55.534436+00:00
[2025-06-03T04:21:57.022+0000] {standard_task_runner.py:72} INFO - Started process 6353 to run task
[2025-06-03T04:21:57.024+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'curry_medallion_etl', 'submit_bronze_etl_job', 'manual__2025-06-03T04:21:55.534436+00:00', '--job-id', '46', '--raw', '--subdir', 'DAGS_FOLDER/medallion_sales_etl_dag.py', '--cfg-path', '/tmp/tmp4zlr05fk']
[2025-06-03T04:21:57.026+0000] {standard_task_runner.py:105} INFO - Job 46: Subtask submit_bronze_etl_job
[2025-06-03T04:21:57.056+0000] {task_command.py:467} INFO - Running <TaskInstance: curry_medallion_etl.submit_bronze_etl_job manual__2025-06-03T04:21:55.534436+00:00 [running]> on host 083f94398a74
[2025-06-03T04:21:57.109+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='qdinh18' AIRFLOW_CTX_DAG_ID='curry_medallion_etl' AIRFLOW_CTX_TASK_ID='submit_bronze_etl_job' AIRFLOW_CTX_EXECUTION_DATE='2025-06-03T04:21:55.534436+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-06-03T04:21:55.534436+00:00'
[2025-06-03T04:21:57.109+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-06-03T04:21:57.135+0000] {base.py:84} INFO - Retrieving connection 'spark_default'
[2025-06-03T04:21:57.137+0000] {spark_submit.py:474} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.executor.memory=2g --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minioadmin --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.sql.warehouse.dir=s3a://mybucket/warehouse --conf spark.sql.catalog.default_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --conf spark.delta.logStore.class=org.apache.spark.sql.delta.storage.HDFSLogStore --conf spark.sql.parquet.compression.codec=snappy --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,io.delta:delta-core_2.12:2.2.0,org.apache.spark:spark-hadoop-cloud_2.12:3.3.0 --name bronze_sc_stats_etl --verbose --queue root.default --deploy-mode client /opt/***/dags/bronze_ingestion_sc_stats_etl.py --output_path s3a://mybucket/bronze/curry_raw_data/
[2025-06-03T04:21:58.020+0000] {spark_submit.py:644} INFO - Using properties file: null
[2025-06-03T04:21:58.080+0000] {spark_submit.py:644} INFO - Parsed arguments:
[2025-06-03T04:21:58.080+0000] {spark_submit.py:644} INFO - master                  spark://spark-master:7077
[2025-06-03T04:21:58.080+0000] {spark_submit.py:644} INFO - remote                  null
[2025-06-03T04:21:58.080+0000] {spark_submit.py:644} INFO - deployMode              client
[2025-06-03T04:21:58.080+0000] {spark_submit.py:644} INFO - executorMemory          2g
[2025-06-03T04:21:58.081+0000] {spark_submit.py:644} INFO - executorCores           null
[2025-06-03T04:21:58.081+0000] {spark_submit.py:644} INFO - totalExecutorCores      null
[2025-06-03T04:21:58.081+0000] {spark_submit.py:644} INFO - propertiesFile          null
[2025-06-03T04:21:58.081+0000] {spark_submit.py:644} INFO - driverMemory            null
[2025-06-03T04:21:58.081+0000] {spark_submit.py:644} INFO - driverCores             null
[2025-06-03T04:21:58.081+0000] {spark_submit.py:644} INFO - driverExtraClassPath    null
[2025-06-03T04:21:58.081+0000] {spark_submit.py:644} INFO - driverExtraLibraryPath  null
[2025-06-03T04:21:58.081+0000] {spark_submit.py:644} INFO - driverExtraJavaOptions  null
[2025-06-03T04:21:58.082+0000] {spark_submit.py:644} INFO - supervise               false
[2025-06-03T04:21:58.082+0000] {spark_submit.py:644} INFO - queue                   root.default
[2025-06-03T04:21:58.082+0000] {spark_submit.py:644} INFO - numExecutors            null
[2025-06-03T04:21:58.082+0000] {spark_submit.py:644} INFO - files                   null
[2025-06-03T04:21:58.082+0000] {spark_submit.py:644} INFO - pyFiles                 null
[2025-06-03T04:21:58.082+0000] {spark_submit.py:644} INFO - archives                null
[2025-06-03T04:21:58.083+0000] {spark_submit.py:644} INFO - mainClass               null
[2025-06-03T04:21:58.083+0000] {spark_submit.py:644} INFO - primaryResource         file:/opt/***/dags/bronze_ingestion_sc_stats_etl.py
[2025-06-03T04:21:58.083+0000] {spark_submit.py:644} INFO - name                    bronze_sc_stats_etl
[2025-06-03T04:21:58.083+0000] {spark_submit.py:644} INFO - childArgs               [--output_path s3a://mybucket/bronze/curry_raw_data/]
[2025-06-03T04:21:58.084+0000] {spark_submit.py:644} INFO - jars                    null
[2025-06-03T04:21:58.084+0000] {spark_submit.py:644} INFO - packages                org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,io.delta:delta-core_2.12:2.2.0,org.apache.spark:spark-hadoop-cloud_2.12:3.3.0
[2025-06-03T04:21:58.084+0000] {spark_submit.py:644} INFO - packagesExclusions      null
[2025-06-03T04:21:58.084+0000] {spark_submit.py:644} INFO - repositories            null
[2025-06-03T04:21:58.084+0000] {spark_submit.py:644} INFO - verbose                 true
[2025-06-03T04:21:58.084+0000] {spark_submit.py:644} INFO - 
[2025-06-03T04:21:58.085+0000] {spark_submit.py:644} INFO - Spark properties used, including those specified through
[2025-06-03T04:21:58.085+0000] {spark_submit.py:644} INFO - --conf and those from the properties file null:
[2025-06-03T04:21:58.085+0000] {spark_submit.py:644} INFO - (spark.delta.logStore.class,org.apache.spark.sql.delta.storage.HDFSLogStore)
[2025-06-03T04:21:58.085+0000] {spark_submit.py:644} INFO - (spark.executor.memory,2g)
[2025-06-03T04:21:58.085+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.access.key,*********(redacted))
[2025-06-03T04:21:58.085+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.aws.credentials.provider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider)
[2025-06-03T04:21:58.085+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.connection.ssl.enabled,false)
[2025-06-03T04:21:58.086+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.endpoint,http://minio:9000)
[2025-06-03T04:21:58.086+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)
[2025-06-03T04:21:58.086+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.path.style.access,true)
[2025-06-03T04:21:58.086+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.secret.key,*********(redacted))
[2025-06-03T04:21:58.086+0000] {spark_submit.py:644} INFO - (spark.sql.catalog.default_catalog,org.apache.spark.sql.delta.catalog.DeltaCatalog)
[2025-06-03T04:21:58.086+0000] {spark_submit.py:644} INFO - (spark.sql.catalog.spark_catalog,org.apache.spark.sql.delta.catalog.DeltaCatalog)
[2025-06-03T04:21:58.086+0000] {spark_submit.py:644} INFO - (spark.sql.extensions,io.delta.sql.DeltaSparkSessionExtension)
[2025-06-03T04:21:58.086+0000] {spark_submit.py:644} INFO - (spark.sql.parquet.compression.codec,snappy)
[2025-06-03T04:21:58.086+0000] {spark_submit.py:644} INFO - (spark.sql.warehouse.dir,s3a://mybucket/warehouse)
[2025-06-03T04:21:58.087+0000] {spark_submit.py:644} INFO - 
[2025-06-03T04:21:58.087+0000] {spark_submit.py:644} INFO - 
[2025-06-03T04:21:58.161+0000] {spark_submit.py:644} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-06-03T04:21:58.209+0000] {spark_submit.py:644} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2025-06-03T04:21:58.209+0000] {spark_submit.py:644} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2025-06-03T04:21:58.211+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2025-06-03T04:21:58.211+0000] {spark_submit.py:644} INFO - com.amazonaws#aws-java-sdk-bundle added as a dependency
[2025-06-03T04:21:58.212+0000] {spark_submit.py:644} INFO - io.delta#delta-core_2.12 added as a dependency
[2025-06-03T04:21:58.212+0000] {spark_submit.py:644} INFO - org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency
[2025-06-03T04:21:58.212+0000] {spark_submit.py:644} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-c24e26ee-3c0d-49c5-9810-c7a19f4e3721;1.0
[2025-06-03T04:21:58.212+0000] {spark_submit.py:644} INFO - confs: [default]
[2025-06-03T04:21:58.316+0000] {spark_submit.py:644} INFO - found org.apache.hadoop#hadoop-aws;3.3.4 in central
[2025-06-03T04:21:58.329+0000] {spark_submit.py:644} INFO - found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central
[2025-06-03T04:21:58.342+0000] {spark_submit.py:644} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2025-06-03T04:21:59.599+0000] {spark_submit.py:644} INFO - found io.delta#delta-core_2.12;2.2.0 in central
[2025-06-03T04:21:59.985+0000] {spark_submit.py:644} INFO - found io.delta#delta-storage;2.2.0 in central
[2025-06-03T04:21:59.996+0000] {spark_submit.py:644} INFO - found org.antlr#antlr4-runtime;4.8 in central
[2025-06-03T04:22:03.267+0000] {spark_submit.py:644} INFO - found org.apache.spark#spark-hadoop-cloud_2.12;3.3.0 in central
[2025-06-03T04:22:06.112+0000] {spark_submit.py:644} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.2 in central
[2025-06-03T04:22:06.520+0000] {spark_submit.py:644} INFO - found org.apache.hadoop#hadoop-client-api;3.3.2 in central
[2025-06-03T04:22:06.896+0000] {spark_submit.py:644} INFO - found org.xerial.snappy#snappy-java;1.1.8.4 in central
[2025-06-03T04:22:08.451+0000] {spark_submit.py:644} INFO - found org.slf4j#slf4j-api;1.7.32 in central
[2025-06-03T04:22:11.174+0000] {spark_submit.py:644} INFO - found commons-logging#commons-logging;1.1.3 in central
[2025-06-03T04:22:11.556+0000] {spark_submit.py:644} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-06-03T04:22:11.959+0000] {spark_submit.py:644} INFO - found org.apache.hadoop#hadoop-openstack;3.3.2 in central
[2025-06-03T04:22:12.351+0000] {spark_submit.py:644} INFO - found org.apache.hadoop#hadoop-annotations;3.3.2 in central
[2025-06-03T04:22:15.825+0000] {spark_submit.py:644} INFO - found org.apache.httpcomponents#httpcore;4.4.14 in central
[2025-06-03T04:22:18.592+0000] {spark_submit.py:644} INFO - found com.fasterxml.jackson.core#jackson-annotations;2.13.3 in central
[2025-06-03T04:22:21.359+0000] {spark_submit.py:644} INFO - found com.fasterxml.jackson.core#jackson-databind;2.13.3 in central
[2025-06-03T04:22:21.739+0000] {spark_submit.py:644} INFO - found com.fasterxml.jackson.core#jackson-core;2.13.3 in central
[2025-06-03T04:22:22.129+0000] {spark_submit.py:644} INFO - found joda-time#joda-time;2.10.13 in central
[2025-06-03T04:22:23.814+0000] {spark_submit.py:644} INFO - found com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.13.3 in central
[2025-06-03T04:22:25.343+0000] {spark_submit.py:644} INFO - found org.apache.httpcomponents#httpclient;4.5.13 in central
[2025-06-03T04:22:28.218+0000] {spark_submit.py:644} INFO - found commons-codec#commons-codec;1.15 in central
[2025-06-03T04:22:28.631+0000] {spark_submit.py:644} INFO - found org.apache.hadoop#hadoop-azure;3.3.2 in central
[2025-06-03T04:22:28.998+0000] {spark_submit.py:644} INFO - found com.microsoft.azure#azure-storage;7.0.1 in central
[2025-06-03T04:22:30.504+0000] {spark_submit.py:644} INFO - found com.microsoft.azure#azure-keyvault-core;1.0.0 in central
[2025-06-03T04:22:32.111+0000] {spark_submit.py:644} INFO - found org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central
[2025-06-03T04:22:32.479+0000] {spark_submit.py:644} INFO - found org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central
[2025-06-03T04:22:32.851+0000] {spark_submit.py:644} INFO - found org.codehaus.jackson#jackson-core-asl;1.9.13 in central
[2025-06-03T04:22:33.242+0000] {spark_submit.py:644} INFO - found org.apache.hadoop#hadoop-cloud-storage;3.3.2 in central
[2025-06-03T04:22:33.634+0000] {spark_submit.py:644} INFO - found org.apache.hadoop#hadoop-aliyun;3.3.2 in central
[2025-06-03T04:22:34.007+0000] {spark_submit.py:644} INFO - found com.aliyun.oss#aliyun-sdk-oss;3.13.0 in central
[2025-06-03T04:22:34.377+0000] {spark_submit.py:644} INFO - found org.jdom#jdom2;2.0.6 in central
[2025-06-03T04:22:34.750+0000] {spark_submit.py:644} INFO - found org.codehaus.jettison#jettison;1.1 in central
[2025-06-03T04:22:35.116+0000] {spark_submit.py:644} INFO - found stax#stax-api;1.0.1 in central
[2025-06-03T04:22:35.489+0000] {spark_submit.py:644} INFO - found com.aliyun#aliyun-java-sdk-core;4.5.10 in central
[2025-06-03T04:22:37.743+0000] {spark_submit.py:644} INFO - found com.google.code.gson#gson;2.8.9 in central
[2025-06-03T04:22:39.484+0000] {spark_submit.py:644} INFO - found javax.xml.bind#jaxb-api;2.2.11 in central
[2025-06-03T04:22:39.870+0000] {spark_submit.py:644} INFO - found org.ini4j#ini4j;0.5.4 in central
[2025-06-03T04:22:41.629+0000] {spark_submit.py:644} INFO - found io.opentracing#opentracing-api;0.33.0 in central
[2025-06-03T04:22:42.011+0000] {spark_submit.py:644} INFO - found io.opentracing#opentracing-util;0.33.0 in central
[2025-06-03T04:22:42.387+0000] {spark_submit.py:644} INFO - found io.opentracing#opentracing-noop;0.33.0 in central
[2025-06-03T04:22:42.762+0000] {spark_submit.py:644} INFO - found com.aliyun#aliyun-java-sdk-ram;3.1.0 in central
[2025-06-03T04:22:43.136+0000] {spark_submit.py:644} INFO - found com.aliyun#aliyun-java-sdk-kms;2.11.0 in central
[2025-06-03T04:22:43.532+0000] {spark_submit.py:644} INFO - found org.apache.hadoop#hadoop-azure-datalake;3.3.2 in central
[2025-06-03T04:22:43.906+0000] {spark_submit.py:644} INFO - found com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central
[2025-06-03T04:22:44.298+0000] {spark_submit.py:644} INFO - found org.apache.hadoop#hadoop-cos;3.3.2 in central
[2025-06-03T04:22:44.663+0000] {spark_submit.py:644} INFO - found com.qcloud#cos_api-bundle;5.6.19 in central
[2025-06-03T04:22:52.532+0000] {spark_submit.py:644} INFO - found org.eclipse.jetty#jetty-util-ajax;9.4.46.v20220331 in central
[2025-06-03T04:22:52.919+0000] {spark_submit.py:644} INFO - found org.eclipse.jetty#jetty-util;9.4.46.v20220331 in central
[2025-06-03T04:22:53.302+0000] {spark_submit.py:644} INFO - found org.spark-project.spark#unused;1.0.0 in central
[2025-06-03T04:22:53.495+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.2.0/delta-core_2.12-2.2.0.jar ...
[2025-06-03T04:22:54.439+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] io.delta#delta-core_2.12;2.2.0!delta-core_2.12.jar (1128ms)
[2025-06-03T04:22:54.682+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.12/3.3.0/spark-hadoop-cloud_2.12-3.3.0.jar ...
[2025-06-03T04:22:54.880+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.spark#spark-hadoop-cloud_2.12;3.3.0!spark-hadoop-cloud_2.12.jar (440ms)
[2025-06-03T04:22:55.063+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.2.0/delta-storage-2.2.0.jar ...
[2025-06-03T04:22:55.249+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] io.delta#delta-storage;2.2.0!delta-storage.jar (369ms)
[2025-06-03T04:22:55.432+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...
[2025-06-03T04:22:56.987+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (1737ms)
[2025-06-03T04:22:57.168+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/3.3.2/hadoop-openstack-3.3.2.jar ...
[2025-06-03T04:22:57.351+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-openstack;3.3.2!hadoop-openstack.jar (364ms)
[2025-06-03T04:22:57.534+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.10.13/joda-time-2.10.13.jar ...
[2025-06-03T04:22:57.736+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] joda-time#joda-time;2.10.13!joda-time.jar (385ms)
[2025-06-03T04:22:57.922+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.13.3/jackson-databind-2.13.3.jar ...
[2025-06-03T04:22:58.123+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.13.3!jackson-databind.jar(bundle) (386ms)
[2025-06-03T04:22:58.305+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.13.3/jackson-annotations-2.13.3.jar ...
[2025-06-03T04:22:58.490+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.13.3!jackson-annotations.jar(bundle) (366ms)
[2025-06-03T04:22:58.672+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.13.3/jackson-dataformat-cbor-2.13.3.jar ...
[2025-06-03T04:22:58.869+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.13.3!jackson-dataformat-cbor.jar(bundle) (378ms)
[2025-06-03T04:22:59.052+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar ...
[2025-06-03T04:22:59.246+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.13!httpclient.jar (376ms)
[2025-06-03T04:22:59.426+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.14/httpcore-4.4.14.jar ...
[2025-06-03T04:22:59.615+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.14!httpcore.jar (369ms)
[2025-06-03T04:22:59.795+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.2/hadoop-azure-3.3.2.jar ...
[2025-06-03T04:22:59.991+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-azure;3.3.2!hadoop-azure.jar (375ms)
[2025-06-03T04:23:00.172+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-cloud-storage/3.3.2/hadoop-cloud-storage-3.3.2.jar ...
[2025-06-03T04:23:00.353+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-cloud-storage;3.3.2!hadoop-cloud-storage.jar (361ms)
[2025-06-03T04:23:00.535+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util-ajax/9.4.46.v20220331/jetty-util-ajax-9.4.46.v20220331.jar ...
[2025-06-03T04:23:00.725+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.eclipse.jetty#jetty-util-ajax;9.4.46.v20220331!jetty-util-ajax.jar (370ms)
[2025-06-03T04:23:00.904+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
[2025-06-03T04:23:01.085+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (360ms)
[2025-06-03T04:23:01.328+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...
[2025-06-03T04:23:02.294+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (1209ms)
[2025-06-03T04:23:02.480+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...
[2025-06-03T04:23:02.684+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (390ms)
[2025-06-03T04:23:02.862+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...
[2025-06-03T04:23:03.043+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (358ms)
[2025-06-03T04:23:03.226+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2025-06-03T04:23:03.407+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (364ms)
[2025-06-03T04:23:03.589+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2025-06-03T04:23:03.774+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (366ms)
[2025-06-03T04:23:03.956+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/3.3.2/hadoop-annotations-3.3.2.jar ...
[2025-06-03T04:23:04.143+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;3.3.2!hadoop-annotations.jar (368ms)
[2025-06-03T04:23:04.328+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.13.3/jackson-core-2.13.3.jar ...
[2025-06-03T04:23:04.520+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.13.3!jackson-core.jar(bundle) (377ms)
[2025-06-03T04:23:04.704+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.15/commons-codec-1.15.jar ...
[2025-06-03T04:23:04.892+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] commons-codec#commons-codec;1.15!commons-codec.jar (371ms)
[2025-06-03T04:23:05.076+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/7.0.1/azure-storage-7.0.1.jar ...
[2025-06-03T04:23:05.282+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.microsoft.azure#azure-storage;7.0.1!azure-storage.jar (389ms)
[2025-06-03T04:23:05.464+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.1.1/hadoop-shaded-guava-1.1.1.jar ...
[2025-06-03T04:23:05.684+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1!hadoop-shaded-guava.jar (402ms)
[2025-06-03T04:23:05.863+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...
[2025-06-03T04:23:06.054+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (370ms)
[2025-06-03T04:23:06.234+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...
[2025-06-03T04:23:06.419+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (364ms)
[2025-06-03T04:23:06.602+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/microsoft/azure/azure-keyvault-core/1.0.0/azure-keyvault-core-1.0.0.jar ...
[2025-06-03T04:23:06.812+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.microsoft.azure#azure-keyvault-core;1.0.0!azure-keyvault-core.jar (393ms)
[2025-06-03T04:23:07.058+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aliyun/3.3.2/hadoop-aliyun-3.3.2.jar ...
[2025-06-03T04:23:07.247+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-aliyun;3.3.2!hadoop-aliyun.jar (433ms)
[2025-06-03T04:23:07.430+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.3.2/hadoop-azure-datalake-3.3.2.jar ...
[2025-06-03T04:23:07.614+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-azure-datalake;3.3.2!hadoop-azure-datalake.jar (366ms)
[2025-06-03T04:23:07.794+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-cos/3.3.2/hadoop-cos-3.3.2.jar ...
[2025-06-03T04:23:07.981+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-cos;3.3.2!hadoop-cos.jar (367ms)
[2025-06-03T04:23:08.248+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/aliyun/oss/aliyun-sdk-oss/3.13.0/aliyun-sdk-oss-3.13.0.jar ...
[2025-06-03T04:23:08.464+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.aliyun.oss#aliyun-sdk-oss;3.13.0!aliyun-sdk-oss.jar (483ms)
[2025-06-03T04:23:08.649+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/jdom/jdom2/2.0.6/jdom2-2.0.6.jar ...
[2025-06-03T04:23:08.837+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.jdom#jdom2;2.0.6!jdom2.jar (372ms)
[2025-06-03T04:23:09.020+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar ...
[2025-06-03T04:23:09.203+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.codehaus.jettison#jettison;1.1!jettison.jar(bundle) (366ms)
[2025-06-03T04:23:09.489+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/aliyun/aliyun-java-sdk-core/4.5.10/aliyun-java-sdk-core-4.5.10.jar ...
[2025-06-03T04:23:09.682+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.aliyun#aliyun-java-sdk-core;4.5.10!aliyun-java-sdk-core.jar (479ms)
[2025-06-03T04:23:09.866+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/aliyun/aliyun-java-sdk-ram/3.1.0/aliyun-java-sdk-ram-3.1.0.jar ...
[2025-06-03T04:23:10.058+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.aliyun#aliyun-java-sdk-ram;3.1.0!aliyun-java-sdk-ram.jar (376ms)
[2025-06-03T04:23:10.239+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/aliyun/aliyun-java-sdk-kms/2.11.0/aliyun-java-sdk-kms-2.11.0.jar ...
[2025-06-03T04:23:10.426+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.aliyun#aliyun-java-sdk-kms;2.11.0!aliyun-java-sdk-kms.jar (367ms)
[2025-06-03T04:23:10.606+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/stax/stax-api/1.0.1/stax-api-1.0.1.jar ...
[2025-06-03T04:23:10.871+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] stax#stax-api;1.0.1!stax-api.jar (445ms)
[2025-06-03T04:23:11.064+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.8.9/gson-2.8.9.jar ...
[2025-06-03T04:23:11.256+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.google.code.gson#gson;2.8.9!gson.jar (383ms)
[2025-06-03T04:23:11.437+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar ...
[2025-06-03T04:23:11.622+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.11!jaxb-api.jar (366ms)
[2025-06-03T04:23:11.806+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/ini4j/ini4j/0.5.4/ini4j-0.5.4.jar ...
[2025-06-03T04:23:11.991+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.ini4j#ini4j;0.5.4!ini4j.jar (369ms)
[2025-06-03T04:23:12.170+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/io/opentracing/opentracing-api/0.33.0/opentracing-api-0.33.0.jar ...
[2025-06-03T04:23:12.445+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] io.opentracing#opentracing-api;0.33.0!opentracing-api.jar (452ms)
[2025-06-03T04:23:12.639+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/io/opentracing/opentracing-util/0.33.0/opentracing-util-0.33.0.jar ...
[2025-06-03T04:23:12.828+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] io.opentracing#opentracing-util;0.33.0!opentracing-util.jar (382ms)
[2025-06-03T04:23:13.011+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/io/opentracing/opentracing-noop/0.33.0/opentracing-noop-0.33.0.jar ...
[2025-06-03T04:23:13.196+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] io.opentracing#opentracing-noop;0.33.0!opentracing-noop.jar (368ms)
[2025-06-03T04:23:13.381+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/microsoft/azure/azure-data-lake-store-sdk/2.3.9/azure-data-lake-store-sdk-2.3.9.jar ...
[2025-06-03T04:23:13.575+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.microsoft.azure#azure-data-lake-store-sdk;2.3.9!azure-data-lake-store-sdk.jar (378ms)
[2025-06-03T04:23:13.760+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/com/qcloud/cos_api-bundle/5.6.19/cos_api-bundle-5.6.19.jar ...
[2025-06-03T04:23:14.841+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] com.qcloud#cos_api-bundle;5.6.19!cos_api-bundle.jar (1265ms)
[2025-06-03T04:23:15.062+0000] {spark_submit.py:644} INFO - downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.46.v20220331/jetty-util-9.4.46.v20220331.jar ...
[2025-06-03T04:23:15.275+0000] {spark_submit.py:644} INFO - [SUCCESSFUL ] org.eclipse.jetty#jetty-util;9.4.46.v20220331!jetty-util.jar (433ms)
[2025-06-03T04:23:15.275+0000] {spark_submit.py:644} INFO - :: resolution report :: resolve 55098ms :: artifacts dl 21965ms
[2025-06-03T04:23:15.276+0000] {spark_submit.py:644} INFO - :: modules in use:
[2025-06-03T04:23:15.276+0000] {spark_submit.py:644} INFO - com.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]
[2025-06-03T04:23:15.277+0000] {spark_submit.py:644} INFO - com.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]
[2025-06-03T04:23:15.277+0000] {spark_submit.py:644} INFO - com.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]
[2025-06-03T04:23:15.278+0000] {spark_submit.py:644} INFO - com.aliyun.oss#aliyun-sdk-oss;3.13.0 from central in [default]
[2025-06-03T04:23:15.278+0000] {spark_submit.py:644} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]
[2025-06-03T04:23:15.278+0000] {spark_submit.py:644} INFO - com.fasterxml.jackson.core#jackson-annotations;2.13.3 from central in [default]
[2025-06-03T04:23:15.279+0000] {spark_submit.py:644} INFO - com.fasterxml.jackson.core#jackson-core;2.13.3 from central in [default]
[2025-06-03T04:23:15.279+0000] {spark_submit.py:644} INFO - com.fasterxml.jackson.core#jackson-databind;2.13.3 from central in [default]
[2025-06-03T04:23:15.279+0000] {spark_submit.py:644} INFO - com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.13.3 from central in [default]
[2025-06-03T04:23:15.280+0000] {spark_submit.py:644} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2025-06-03T04:23:15.280+0000] {spark_submit.py:644} INFO - com.google.code.gson#gson;2.8.9 from central in [default]
[2025-06-03T04:23:15.281+0000] {spark_submit.py:644} INFO - com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]
[2025-06-03T04:23:15.281+0000] {spark_submit.py:644} INFO - com.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]
[2025-06-03T04:23:15.281+0000] {spark_submit.py:644} INFO - com.microsoft.azure#azure-storage;7.0.1 from central in [default]
[2025-06-03T04:23:15.282+0000] {spark_submit.py:644} INFO - com.qcloud#cos_api-bundle;5.6.19 from central in [default]
[2025-06-03T04:23:15.282+0000] {spark_submit.py:644} INFO - commons-codec#commons-codec;1.15 from central in [default]
[2025-06-03T04:23:15.282+0000] {spark_submit.py:644} INFO - commons-logging#commons-logging;1.1.3 from central in [default]
[2025-06-03T04:23:15.283+0000] {spark_submit.py:644} INFO - io.delta#delta-core_2.12;2.2.0 from central in [default]
[2025-06-03T04:23:15.283+0000] {spark_submit.py:644} INFO - io.delta#delta-storage;2.2.0 from central in [default]
[2025-06-03T04:23:15.283+0000] {spark_submit.py:644} INFO - io.opentracing#opentracing-api;0.33.0 from central in [default]
[2025-06-03T04:23:15.284+0000] {spark_submit.py:644} INFO - io.opentracing#opentracing-noop;0.33.0 from central in [default]
[2025-06-03T04:23:15.284+0000] {spark_submit.py:644} INFO - io.opentracing#opentracing-util;0.33.0 from central in [default]
[2025-06-03T04:23:15.284+0000] {spark_submit.py:644} INFO - javax.xml.bind#jaxb-api;2.2.11 from central in [default]
[2025-06-03T04:23:15.285+0000] {spark_submit.py:644} INFO - joda-time#joda-time;2.10.13 from central in [default]
[2025-06-03T04:23:15.285+0000] {spark_submit.py:644} INFO - org.antlr#antlr4-runtime;4.8 from central in [default]
[2025-06-03T04:23:15.285+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-aliyun;3.3.2 from central in [default]
[2025-06-03T04:23:15.285+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-annotations;3.3.2 from central in [default]
[2025-06-03T04:23:15.286+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-aws;3.3.4 from central in [default]
[2025-06-03T04:23:15.286+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-azure;3.3.2 from central in [default]
[2025-06-03T04:23:15.286+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-azure-datalake;3.3.2 from central in [default]
[2025-06-03T04:23:15.287+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]
[2025-06-03T04:23:15.287+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]
[2025-06-03T04:23:15.287+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-cloud-storage;3.3.2 from central in [default]
[2025-06-03T04:23:15.287+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-cos;3.3.2 from central in [default]
[2025-06-03T04:23:15.288+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-openstack;3.3.2 from central in [default]
[2025-06-03T04:23:15.288+0000] {spark_submit.py:644} INFO - org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]
[2025-06-03T04:23:15.288+0000] {spark_submit.py:644} INFO - org.apache.httpcomponents#httpclient;4.5.13 from central in [default]
[2025-06-03T04:23:15.289+0000] {spark_submit.py:644} INFO - org.apache.httpcomponents#httpcore;4.4.14 from central in [default]
[2025-06-03T04:23:15.289+0000] {spark_submit.py:644} INFO - org.apache.spark#spark-hadoop-cloud_2.12;3.3.0 from central in [default]
[2025-06-03T04:23:15.289+0000] {spark_submit.py:644} INFO - org.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]
[2025-06-03T04:23:15.289+0000] {spark_submit.py:644} INFO - org.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]
[2025-06-03T04:23:15.290+0000] {spark_submit.py:644} INFO - org.codehaus.jettison#jettison;1.1 from central in [default]
[2025-06-03T04:23:15.290+0000] {spark_submit.py:644} INFO - org.eclipse.jetty#jetty-util;9.4.46.v20220331 from central in [default]
[2025-06-03T04:23:15.290+0000] {spark_submit.py:644} INFO - org.eclipse.jetty#jetty-util-ajax;9.4.46.v20220331 from central in [default]
[2025-06-03T04:23:15.291+0000] {spark_submit.py:644} INFO - org.ini4j#ini4j;0.5.4 from central in [default]
[2025-06-03T04:23:15.291+0000] {spark_submit.py:644} INFO - org.jdom#jdom2;2.0.6 from central in [default]
[2025-06-03T04:23:15.291+0000] {spark_submit.py:644} INFO - org.slf4j#slf4j-api;1.7.32 from central in [default]
[2025-06-03T04:23:15.291+0000] {spark_submit.py:644} INFO - org.spark-project.spark#unused;1.0.0 from central in [default]
[2025-06-03T04:23:15.291+0000] {spark_submit.py:644} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2025-06-03T04:23:15.292+0000] {spark_submit.py:644} INFO - org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
[2025-06-03T04:23:15.292+0000] {spark_submit.py:644} INFO - stax#stax-api;1.0.1 from central in [default]
[2025-06-03T04:23:15.292+0000] {spark_submit.py:644} INFO - :: evicted modules:
[2025-06-03T04:23:15.292+0000] {spark_submit.py:644} INFO - org.apache.hadoop#hadoop-aws;3.3.2 by [org.apache.hadoop#hadoop-aws;3.3.4] in [default]
[2025-06-03T04:23:15.293+0000] {spark_submit.py:644} INFO - org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 by [org.eclipse.jetty#jetty-util-ajax;9.4.46.v20220331] in [default]
[2025-06-03T04:23:15.293+0000] {spark_submit.py:644} INFO - ---------------------------------------------------------------------
[2025-06-03T04:23:15.293+0000] {spark_submit.py:644} INFO - |                  |            modules            ||   artifacts   |
[2025-06-03T04:23:15.294+0000] {spark_submit.py:644} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-06-03T04:23:15.294+0000] {spark_submit.py:644} INFO - ---------------------------------------------------------------------
[2025-06-03T04:23:15.294+0000] {spark_submit.py:644} INFO - |      default     |   53  |   47  |   47  |   2   ||   51  |   47  |
[2025-06-03T04:23:15.294+0000] {spark_submit.py:644} INFO - ---------------------------------------------------------------------
[2025-06-03T04:23:15.295+0000] {spark_submit.py:644} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-c24e26ee-3c0d-49c5-9810-c7a19f4e3721
[2025-06-03T04:23:15.295+0000] {spark_submit.py:644} INFO - confs: [default]
[2025-06-03T04:23:15.381+0000] {spark_submit.py:644} INFO - 47 artifacts copied, 4 already retrieved (76272kB/95ms)
[2025-06-03T04:23:15.529+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-06-03T04:23:15.664+0000] {spark_submit.py:644} INFO - Main class:
[2025-06-03T04:23:15.665+0000] {spark_submit.py:644} INFO - org.apache.spark.deploy.PythonRunner
[2025-06-03T04:23:15.665+0000] {spark_submit.py:644} INFO - Arguments:
[2025-06-03T04:23:15.665+0000] {spark_submit.py:644} INFO - file:/opt/***/dags/bronze_ingestion_sc_stats_etl.py
[2025-06-03T04:23:15.666+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///home/***/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/***/.ivy2/jars/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar,file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/***/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-openstack-3.3.2.jar,file:///home/***/.ivy2/jars/joda-time_joda-time-2.10.13.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar,file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar,file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.14.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar,file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar,file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.3.2.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.3.jar,file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-storage-7.0.1.jar,file:///home/***/.ivy2/jars/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar,file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-keyvault-core-1.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aliyun-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cos-3.3.2.jar,file:///home/***/.ivy2/jars/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar,file:///home/***/.ivy2/jars/org.jdom_jdom2-2.0.6.jar,file:///home/***/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-core-4.5.10.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar,file:///home/***/.ivy2/jars/stax_stax-api-1.0.1.jar,file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.8.9.jar,file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar,file:///home/***/.ivy2/jars/org.ini4j_ini4j-0.5.4.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-api-0.33.0.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-util-0.33.0.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-noop-0.33.0.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar,file:///home/***/.ivy2/jars/com.qcloud_cos_api-bundle-5.6.19.jar,file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar
[2025-06-03T04:23:15.666+0000] {spark_submit.py:644} INFO - --output_path
[2025-06-03T04:23:15.666+0000] {spark_submit.py:644} INFO - s3a://mybucket/bronze/curry_raw_data/
[2025-06-03T04:23:15.666+0000] {spark_submit.py:644} INFO - Spark config:
[2025-06-03T04:23:15.666+0000] {spark_submit.py:644} INFO - (spark.app.name,bronze_sc_stats_etl)
[2025-06-03T04:23:15.667+0000] {spark_submit.py:644} INFO - (spark.app.submitTime,1748924595657)
[2025-06-03T04:23:15.667+0000] {spark_submit.py:644} INFO - (spark.delta.logStore.class,org.apache.spark.sql.delta.storage.HDFSLogStore)
[2025-06-03T04:23:15.667+0000] {spark_submit.py:644} INFO - (spark.executor.memory,2g)
[2025-06-03T04:23:15.667+0000] {spark_submit.py:644} INFO - (spark.files,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///home/***/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/***/.ivy2/jars/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar,file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/***/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-openstack-3.3.2.jar,file:///home/***/.ivy2/jars/joda-time_joda-time-2.10.13.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar,file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar,file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.14.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar,file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar,file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.3.2.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.3.jar,file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-storage-7.0.1.jar,file:///home/***/.ivy2/jars/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar,file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-keyvault-core-1.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aliyun-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cos-3.3.2.jar,file:///home/***/.ivy2/jars/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar,file:///home/***/.ivy2/jars/org.jdom_jdom2-2.0.6.jar,file:///home/***/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-core-4.5.10.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar,file:///home/***/.ivy2/jars/stax_stax-api-1.0.1.jar,file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.8.9.jar,file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar,file:///home/***/.ivy2/jars/org.ini4j_ini4j-0.5.4.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-api-0.33.0.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-util-0.33.0.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-noop-0.33.0.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar,file:///home/***/.ivy2/jars/com.qcloud_cos_api-bundle-5.6.19.jar,file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar)
[2025-06-03T04:23:15.667+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.access.key,*********(redacted))
[2025-06-03T04:23:15.667+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.aws.credentials.provider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider)
[2025-06-03T04:23:15.667+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.connection.ssl.enabled,false)
[2025-06-03T04:23:15.668+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.endpoint,http://minio:9000)
[2025-06-03T04:23:15.668+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)
[2025-06-03T04:23:15.668+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.path.style.access,true)
[2025-06-03T04:23:15.668+0000] {spark_submit.py:644} INFO - (spark.hadoop.fs.s3a.secret.key,*********(redacted))
[2025-06-03T04:23:15.668+0000] {spark_submit.py:644} INFO - (spark.jars,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///home/***/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/***/.ivy2/jars/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar,file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/***/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-openstack-3.3.2.jar,file:///home/***/.ivy2/jars/joda-time_joda-time-2.10.13.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar,file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar,file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.14.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar,file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar,file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.3.2.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.3.jar,file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-storage-7.0.1.jar,file:///home/***/.ivy2/jars/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar,file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-keyvault-core-1.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aliyun-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cos-3.3.2.jar,file:///home/***/.ivy2/jars/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar,file:///home/***/.ivy2/jars/org.jdom_jdom2-2.0.6.jar,file:///home/***/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-core-4.5.10.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar,file:///home/***/.ivy2/jars/stax_stax-api-1.0.1.jar,file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.8.9.jar,file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar,file:///home/***/.ivy2/jars/org.ini4j_ini4j-0.5.4.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-api-0.33.0.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-util-0.33.0.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-noop-0.33.0.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar,file:///home/***/.ivy2/jars/com.qcloud_cos_api-bundle-5.6.19.jar,file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar)
[2025-06-03T04:23:15.669+0000] {spark_submit.py:644} INFO - (spark.master,spark://spark-master:7077)
[2025-06-03T04:23:15.669+0000] {spark_submit.py:644} INFO - (spark.repl.local.jars,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,file:///home/***/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/***/.ivy2/jars/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar,file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/***/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-openstack-3.3.2.jar,file:///home/***/.ivy2/jars/joda-time_joda-time-2.10.13.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar,file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar,file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.14.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar,file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar,file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.3.2.jar,file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.3.jar,file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-storage-7.0.1.jar,file:///home/***/.ivy2/jars/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar,file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-keyvault-core-1.0.0.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aliyun-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar,file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cos-3.3.2.jar,file:///home/***/.ivy2/jars/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar,file:///home/***/.ivy2/jars/org.jdom_jdom2-2.0.6.jar,file:///home/***/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-core-4.5.10.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar,file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar,file:///home/***/.ivy2/jars/stax_stax-api-1.0.1.jar,file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.8.9.jar,file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar,file:///home/***/.ivy2/jars/org.ini4j_ini4j-0.5.4.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-api-0.33.0.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-util-0.33.0.jar,file:///home/***/.ivy2/jars/io.opentracing_opentracing-noop-0.33.0.jar,file:///home/***/.ivy2/jars/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar,file:///home/***/.ivy2/jars/com.qcloud_cos_api-bundle-5.6.19.jar,file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar)
[2025-06-03T04:23:15.669+0000] {spark_submit.py:644} INFO - (spark.sql.catalog.default_catalog,org.apache.spark.sql.delta.catalog.DeltaCatalog)
[2025-06-03T04:23:15.669+0000] {spark_submit.py:644} INFO - (spark.sql.catalog.spark_catalog,org.apache.spark.sql.delta.catalog.DeltaCatalog)
[2025-06-03T04:23:15.670+0000] {spark_submit.py:644} INFO - (spark.sql.extensions,io.delta.sql.DeltaSparkSessionExtension)
[2025-06-03T04:23:15.670+0000] {spark_submit.py:644} INFO - (spark.sql.parquet.compression.codec,snappy)
[2025-06-03T04:23:15.670+0000] {spark_submit.py:644} INFO - (spark.sql.warehouse.dir,s3a://mybucket/warehouse)
[2025-06-03T04:23:15.670+0000] {spark_submit.py:644} INFO - (spark.submit.deployMode,client)
[2025-06-03T04:23:15.670+0000] {spark_submit.py:644} INFO - (spark.submit.pyFiles,/home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,/home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar,/home/***/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,/home/***/.ivy2/jars/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar,/home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,/home/***/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,/home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar,/home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar,/home/***/.ivy2/jars/org.apache.hadoop_hadoop-openstack-3.3.2.jar,/home/***/.ivy2/jars/joda-time_joda-time-2.10.13.jar,/home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar,/home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar,/home/***/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar,/home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar,/home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.14.jar,/home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-3.3.2.jar,/home/***/.ivy2/jars/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar,/home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar,/home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,/home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar,/home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar,/home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar,/home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,/home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,/home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.3.2.jar,/home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.3.jar,/home/***/.ivy2/jars/commons-codec_commons-codec-1.15.jar,/home/***/.ivy2/jars/com.microsoft.azure_azure-storage-7.0.1.jar,/home/***/.ivy2/jars/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar,/home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,/home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,/home/***/.ivy2/jars/com.microsoft.azure_azure-keyvault-core-1.0.0.jar,/home/***/.ivy2/jars/org.apache.hadoop_hadoop-aliyun-3.3.2.jar,/home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar,/home/***/.ivy2/jars/org.apache.hadoop_hadoop-cos-3.3.2.jar,/home/***/.ivy2/jars/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar,/home/***/.ivy2/jars/org.jdom_jdom2-2.0.6.jar,/home/***/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar,/home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-core-4.5.10.jar,/home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar,/home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar,/home/***/.ivy2/jars/stax_stax-api-1.0.1.jar,/home/***/.ivy2/jars/com.google.code.gson_gson-2.8.9.jar,/home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar,/home/***/.ivy2/jars/org.ini4j_ini4j-0.5.4.jar,/home/***/.ivy2/jars/io.opentracing_opentracing-api-0.33.0.jar,/home/***/.ivy2/jars/io.opentracing_opentracing-util-0.33.0.jar,/home/***/.ivy2/jars/io.opentracing_opentracing-noop-0.33.0.jar,/home/***/.ivy2/jars/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar,/home/***/.ivy2/jars/com.qcloud_cos_api-bundle-5.6.19.jar,/home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar)
[2025-06-03T04:23:15.670+0000] {spark_submit.py:644} INFO - Classpath elements:
[2025-06-03T04:23:15.671+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-06-03T04:23:15.671+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-06-03T04:23:15.671+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar
[2025-06-03T04:23:15.671+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar
[2025-06-03T04:23:15.672+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-06-03T04:23:15.672+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/io.delta_delta-storage-2.2.0.jar
[2025-06-03T04:23:15.672+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar
[2025-06-03T04:23:15.672+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2025-06-03T04:23:15.672+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-openstack-3.3.2.jar
[2025-06-03T04:23:15.672+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/joda-time_joda-time-2.10.13.jar
[2025-06-03T04:23:15.673+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar
[2025-06-03T04:23:15.673+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar
[2025-06-03T04:23:15.673+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar
[2025-06-03T04:23:15.673+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar
[2025-06-03T04:23:15.673+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.14.jar
[2025-06-03T04:23:15.673+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-3.3.2.jar
[2025-06-03T04:23:15.673+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar
[2025-06-03T04:23:15.674+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar
[2025-06-03T04:23:15.674+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar
[2025-06-03T04:23:15.674+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2025-06-03T04:23:15.674+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2025-06-03T04:23:15.674+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar
[2025-06-03T04:23:15.674+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar
[2025-06-03T04:23:15.674+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-06-03T04:23:15.674+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.3.2.jar
[2025-06-03T04:23:15.675+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.3.jar
[2025-06-03T04:23:15.675+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.15.jar
[2025-06-03T04:23:15.675+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.microsoft.azure_azure-storage-7.0.1.jar
[2025-06-03T04:23:15.675+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar
[2025-06-03T04:23:15.675+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar
[2025-06-03T04:23:15.675+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar
[2025-06-03T04:23:15.676+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.microsoft.azure_azure-keyvault-core-1.0.0.jar
[2025-06-03T04:23:15.676+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aliyun-3.3.2.jar
[2025-06-03T04:23:15.676+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar
[2025-06-03T04:23:15.676+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cos-3.3.2.jar
[2025-06-03T04:23:15.676+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar
[2025-06-03T04:23:15.676+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.jdom_jdom2-2.0.6.jar
[2025-06-03T04:23:15.676+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar
[2025-06-03T04:23:15.676+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-core-4.5.10.jar
[2025-06-03T04:23:15.677+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar
[2025-06-03T04:23:15.677+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar
[2025-06-03T04:23:15.677+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/stax_stax-api-1.0.1.jar
[2025-06-03T04:23:15.677+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.8.9.jar
[2025-06-03T04:23:15.677+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar
[2025-06-03T04:23:15.677+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.ini4j_ini4j-0.5.4.jar
[2025-06-03T04:23:15.677+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/io.opentracing_opentracing-api-0.33.0.jar
[2025-06-03T04:23:15.677+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/io.opentracing_opentracing-util-0.33.0.jar
[2025-06-03T04:23:15.678+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/io.opentracing_opentracing-noop-0.33.0.jar
[2025-06-03T04:23:15.678+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar
[2025-06-03T04:23:15.678+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/com.qcloud_cos_api-bundle-5.6.19.jar
[2025-06-03T04:23:15.678+0000] {spark_submit.py:644} INFO - file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar
[2025-06-03T04:23:15.678+0000] {spark_submit.py:644} INFO - 
[2025-06-03T04:23:15.678+0000] {spark_submit.py:644} INFO - 
[2025-06-03T04:23:16.517+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Running Spark version 3.5.2
[2025-06-03T04:23:16.517+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-06-03T04:23:16.517+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Java version 17.0.15
[2025-06-03T04:23:16.528+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO ResourceUtils: ==============================================================
[2025-06-03T04:23:16.528+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-06-03T04:23:16.528+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO ResourceUtils: ==============================================================
[2025-06-03T04:23:16.529+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Submitted application: BronzeIngestionStephenCurryStats
[2025-06-03T04:23:16.542+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-06-03T04:23:16.546+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO ResourceProfile: Limiting resource is cpu
[2025-06-03T04:23:16.547+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-06-03T04:23:16.577+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SecurityManager: Changing view acls to: ***
[2025-06-03T04:23:16.578+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SecurityManager: Changing modify acls to: ***
[2025-06-03T04:23:16.578+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SecurityManager: Changing view acls groups to:
[2025-06-03T04:23:16.578+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SecurityManager: Changing modify acls groups to:
[2025-06-03T04:23:16.579+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-06-03T04:23:16.703+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO Utils: Successfully started service 'sparkDriver' on port 38887.
[2025-06-03T04:23:16.720+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkEnv: Registering MapOutputTracker
[2025-06-03T04:23:16.738+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkEnv: Registering BlockManagerMaster
[2025-06-03T04:23:16.747+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-06-03T04:23:16.748+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-06-03T04:23:16.750+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-06-03T04:23:16.761+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0a3db648-7ef0-4359-bdba-ad00fab62443
[2025-06-03T04:23:16.769+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-06-03T04:23:16.776+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-06-03T04:23:16.852+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-06-03T04:23:16.889+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-06-03T04:23:16.910+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://083f94398a74:38887/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1748924596513
[2025-06-03T04:23:16.910+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://083f94398a74:38887/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1748924596513
[2025-06-03T04:23:16.910+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar at spark://083f94398a74:38887/jars/io.delta_delta-core_2.12-2.2.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.910+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar at spark://083f94398a74:38887/jars/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.911+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://083f94398a74:38887/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1748924596513
[2025-06-03T04:23:16.911+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.delta_delta-storage-2.2.0.jar at spark://083f94398a74:38887/jars/io.delta_delta-storage-2.2.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.911+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar at spark://083f94398a74:38887/jars/org.antlr_antlr4-runtime-4.8.jar with timestamp 1748924596513
[2025-06-03T04:23:16.911+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://083f94398a74:38887/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:16.911+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-openstack-3.3.2.jar at spark://083f94398a74:38887/jars/org.apache.hadoop_hadoop-openstack-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:16.911+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/joda-time_joda-time-2.10.13.jar at spark://083f94398a74:38887/jars/joda-time_joda-time-2.10.13.jar with timestamp 1748924596513
[2025-06-03T04:23:16.912+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar at spark://083f94398a74:38887/jars/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar with timestamp 1748924596513
[2025-06-03T04:23:16.912+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar at spark://083f94398a74:38887/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar with timestamp 1748924596513
[2025-06-03T04:23:16.912+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar at spark://083f94398a74:38887/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar with timestamp 1748924596513
[2025-06-03T04:23:16.912+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar at spark://083f94398a74:38887/jars/org.apache.httpcomponents_httpclient-4.5.13.jar with timestamp 1748924596513
[2025-06-03T04:23:16.912+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.14.jar at spark://083f94398a74:38887/jars/org.apache.httpcomponents_httpcore-4.4.14.jar with timestamp 1748924596513
[2025-06-03T04:23:16.912+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-3.3.2.jar at spark://083f94398a74:38887/jars/org.apache.hadoop_hadoop-azure-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:16.912+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar at spark://083f94398a74:38887/jars/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:16.912+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar at spark://083f94398a74:38887/jars/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar with timestamp 1748924596513
[2025-06-03T04:23:16.913+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://083f94398a74:38887/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.913+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://083f94398a74:38887/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:16.913+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://083f94398a74:38887/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1748924596513
[2025-06-03T04:23:16.913+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://083f94398a74:38887/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1748924596513
[2025-06-03T04:23:16.913+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://083f94398a74:38887/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1748924596513
[2025-06-03T04:23:16.913+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://083f94398a74:38887/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.913+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.3.2.jar at spark://083f94398a74:38887/jars/org.apache.hadoop_hadoop-annotations-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:16.913+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.3.jar at spark://083f94398a74:38887/jars/com.fasterxml.jackson.core_jackson-core-2.13.3.jar with timestamp 1748924596513
[2025-06-03T04:23:16.913+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.15.jar at spark://083f94398a74:38887/jars/commons-codec_commons-codec-1.15.jar with timestamp 1748924596513
[2025-06-03T04:23:16.914+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.microsoft.azure_azure-storage-7.0.1.jar at spark://083f94398a74:38887/jars/com.microsoft.azure_azure-storage-7.0.1.jar with timestamp 1748924596513
[2025-06-03T04:23:16.914+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar at spark://083f94398a74:38887/jars/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar with timestamp 1748924596513
[2025-06-03T04:23:16.914+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar at spark://083f94398a74:38887/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar with timestamp 1748924596513
[2025-06-03T04:23:16.914+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar at spark://083f94398a74:38887/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar with timestamp 1748924596513
[2025-06-03T04:23:16.914+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.microsoft.azure_azure-keyvault-core-1.0.0.jar at spark://083f94398a74:38887/jars/com.microsoft.azure_azure-keyvault-core-1.0.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.914+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aliyun-3.3.2.jar at spark://083f94398a74:38887/jars/org.apache.hadoop_hadoop-aliyun-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:16.914+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar at spark://083f94398a74:38887/jars/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:16.914+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cos-3.3.2.jar at spark://083f94398a74:38887/jars/org.apache.hadoop_hadoop-cos-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:16.915+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar at spark://083f94398a74:38887/jars/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.915+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.jdom_jdom2-2.0.6.jar at spark://083f94398a74:38887/jars/org.jdom_jdom2-2.0.6.jar with timestamp 1748924596513
[2025-06-03T04:23:16.915+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar at spark://083f94398a74:38887/jars/org.codehaus.jettison_jettison-1.1.jar with timestamp 1748924596513
[2025-06-03T04:23:16.915+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-core-4.5.10.jar at spark://083f94398a74:38887/jars/com.aliyun_aliyun-java-sdk-core-4.5.10.jar with timestamp 1748924596513
[2025-06-03T04:23:16.915+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar at spark://083f94398a74:38887/jars/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.915+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar at spark://083f94398a74:38887/jars/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.915+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/stax_stax-api-1.0.1.jar at spark://083f94398a74:38887/jars/stax_stax-api-1.0.1.jar with timestamp 1748924596513
[2025-06-03T04:23:16.915+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.8.9.jar at spark://083f94398a74:38887/jars/com.google.code.gson_gson-2.8.9.jar with timestamp 1748924596513
[2025-06-03T04:23:16.916+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar at spark://083f94398a74:38887/jars/javax.xml.bind_jaxb-api-2.2.11.jar with timestamp 1748924596513
[2025-06-03T04:23:16.916+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.ini4j_ini4j-0.5.4.jar at spark://083f94398a74:38887/jars/org.ini4j_ini4j-0.5.4.jar with timestamp 1748924596513
[2025-06-03T04:23:16.916+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.opentracing_opentracing-api-0.33.0.jar at spark://083f94398a74:38887/jars/io.opentracing_opentracing-api-0.33.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.916+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.opentracing_opentracing-util-0.33.0.jar at spark://083f94398a74:38887/jars/io.opentracing_opentracing-util-0.33.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.916+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.opentracing_opentracing-noop-0.33.0.jar at spark://083f94398a74:38887/jars/io.opentracing_opentracing-noop-0.33.0.jar with timestamp 1748924596513
[2025-06-03T04:23:16.916+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar at spark://083f94398a74:38887/jars/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar with timestamp 1748924596513
[2025-06-03T04:23:16.916+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.qcloud_cos_api-bundle-5.6.19.jar at spark://083f94398a74:38887/jars/com.qcloud_cos_api-bundle-5.6.19.jar with timestamp 1748924596513
[2025-06-03T04:23:16.916+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar at spark://083f94398a74:38887/jars/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar with timestamp 1748924596513
[2025-06-03T04:23:16.917+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://083f94398a74:38887/files/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1748924596513
[2025-06-03T04:23:16.917+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-06-03T04:23:16.921+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://083f94398a74:38887/files/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1748924596513
[2025-06-03T04:23:16.922+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:16 INFO Utils: Copying /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-06-03T04:23:17.145+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar at spark://083f94398a74:38887/files/io.delta_delta-core_2.12-2.2.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.147+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/io.delta_delta-core_2.12-2.2.0.jar
[2025-06-03T04:23:17.150+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar at spark://083f94398a74:38887/files/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.150+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.spark_spark-hadoop-cloud_2.12-3.3.0.jar
[2025-06-03T04:23:17.153+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://083f94398a74:38887/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1748924596513
[2025-06-03T04:23:17.153+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-06-03T04:23:17.200+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.delta_delta-storage-2.2.0.jar at spark://083f94398a74:38887/files/io.delta_delta-storage-2.2.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.201+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/io.delta_delta-storage-2.2.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/io.delta_delta-storage-2.2.0.jar
[2025-06-03T04:23:17.203+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar at spark://083f94398a74:38887/files/org.antlr_antlr4-runtime-4.8.jar with timestamp 1748924596513
[2025-06-03T04:23:17.203+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.antlr_antlr4-runtime-4.8.jar
[2025-06-03T04:23:17.206+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://083f94398a74:38887/files/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:17.206+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2025-06-03T04:23:17.224+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-openstack-3.3.2.jar at spark://083f94398a74:38887/files/org.apache.hadoop_hadoop-openstack-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:17.225+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-openstack-3.3.2.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop_hadoop-openstack-3.3.2.jar
[2025-06-03T04:23:17.227+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/joda-time_joda-time-2.10.13.jar at spark://083f94398a74:38887/files/joda-time_joda-time-2.10.13.jar with timestamp 1748924596513
[2025-06-03T04:23:17.228+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/joda-time_joda-time-2.10.13.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/joda-time_joda-time-2.10.13.jar
[2025-06-03T04:23:17.231+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar at spark://083f94398a74:38887/files/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar with timestamp 1748924596513
[2025-06-03T04:23:17.231+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.fasterxml.jackson.core_jackson-databind-2.13.3.jar
[2025-06-03T04:23:17.234+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar at spark://083f94398a74:38887/files/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar with timestamp 1748924596513
[2025-06-03T04:23:17.235+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.fasterxml.jackson.core_jackson-annotations-2.13.3.jar
[2025-06-03T04:23:17.237+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar at spark://083f94398a74:38887/files/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar with timestamp 1748924596513
[2025-06-03T04:23:17.237+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.13.3.jar
[2025-06-03T04:23:17.239+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar at spark://083f94398a74:38887/files/org.apache.httpcomponents_httpclient-4.5.13.jar with timestamp 1748924596513
[2025-06-03T04:23:17.239+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.httpcomponents_httpclient-4.5.13.jar
[2025-06-03T04:23:17.242+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.14.jar at spark://083f94398a74:38887/files/org.apache.httpcomponents_httpcore-4.4.14.jar with timestamp 1748924596513
[2025-06-03T04:23:17.242+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.14.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.httpcomponents_httpcore-4.4.14.jar
[2025-06-03T04:23:17.244+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-3.3.2.jar at spark://083f94398a74:38887/files/org.apache.hadoop_hadoop-azure-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:17.244+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-3.3.2.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop_hadoop-azure-3.3.2.jar
[2025-06-03T04:23:17.247+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar at spark://083f94398a74:38887/files/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:17.247+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop_hadoop-cloud-storage-3.3.2.jar
[2025-06-03T04:23:17.249+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar at spark://083f94398a74:38887/files/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar with timestamp 1748924596513
[2025-06-03T04:23:17.249+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.eclipse.jetty_jetty-util-ajax-9.4.46.v20220331.jar
[2025-06-03T04:23:17.251+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://083f94398a74:38887/files/org.spark-project.spark_unused-1.0.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.251+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.spark-project.spark_unused-1.0.0.jar
[2025-06-03T04:23:17.253+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://083f94398a74:38887/files/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:17.254+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2025-06-03T04:23:17.263+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://083f94398a74:38887/files/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1748924596513
[2025-06-03T04:23:17.264+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2025-06-03T04:23:17.268+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://083f94398a74:38887/files/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1748924596513
[2025-06-03T04:23:17.268+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.slf4j_slf4j-api-1.7.32.jar
[2025-06-03T04:23:17.272+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://083f94398a74:38887/files/commons-logging_commons-logging-1.1.3.jar with timestamp 1748924596513
[2025-06-03T04:23:17.272+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/commons-logging_commons-logging-1.1.3.jar
[2025-06-03T04:23:17.274+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://083f94398a74:38887/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.274+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-06-03T04:23:17.276+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.3.2.jar at spark://083f94398a74:38887/files/org.apache.hadoop_hadoop-annotations-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:17.276+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-annotations-3.3.2.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop_hadoop-annotations-3.3.2.jar
[2025-06-03T04:23:17.280+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.3.jar at spark://083f94398a74:38887/files/com.fasterxml.jackson.core_jackson-core-2.13.3.jar with timestamp 1748924596513
[2025-06-03T04:23:17.281+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.13.3.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.fasterxml.jackson.core_jackson-core-2.13.3.jar
[2025-06-03T04:23:17.285+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-codec_commons-codec-1.15.jar at spark://083f94398a74:38887/files/commons-codec_commons-codec-1.15.jar with timestamp 1748924596513
[2025-06-03T04:23:17.286+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/commons-codec_commons-codec-1.15.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/commons-codec_commons-codec-1.15.jar
[2025-06-03T04:23:17.289+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.microsoft.azure_azure-storage-7.0.1.jar at spark://083f94398a74:38887/files/com.microsoft.azure_azure-storage-7.0.1.jar with timestamp 1748924596513
[2025-06-03T04:23:17.289+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.microsoft.azure_azure-storage-7.0.1.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.microsoft.azure_azure-storage-7.0.1.jar
[2025-06-03T04:23:17.291+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar at spark://083f94398a74:38887/files/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar with timestamp 1748924596513
[2025-06-03T04:23:17.292+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop.thirdparty_hadoop-shaded-guava-1.1.1.jar
[2025-06-03T04:23:17.294+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar at spark://083f94398a74:38887/files/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar with timestamp 1748924596513
[2025-06-03T04:23:17.295+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar
[2025-06-03T04:23:17.297+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar at spark://083f94398a74:38887/files/org.codehaus.jackson_jackson-core-asl-1.9.13.jar with timestamp 1748924596513
[2025-06-03T04:23:17.297+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.codehaus.jackson_jackson-core-asl-1.9.13.jar
[2025-06-03T04:23:17.299+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.microsoft.azure_azure-keyvault-core-1.0.0.jar at spark://083f94398a74:38887/files/com.microsoft.azure_azure-keyvault-core-1.0.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.300+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.microsoft.azure_azure-keyvault-core-1.0.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.microsoft.azure_azure-keyvault-core-1.0.0.jar
[2025-06-03T04:23:17.302+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aliyun-3.3.2.jar at spark://083f94398a74:38887/files/org.apache.hadoop_hadoop-aliyun-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:17.303+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aliyun-3.3.2.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop_hadoop-aliyun-3.3.2.jar
[2025-06-03T04:23:17.305+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar at spark://083f94398a74:38887/files/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:17.306+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop_hadoop-azure-datalake-3.3.2.jar
[2025-06-03T04:23:17.308+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-cos-3.3.2.jar at spark://083f94398a74:38887/files/org.apache.hadoop_hadoop-cos-3.3.2.jar with timestamp 1748924596513
[2025-06-03T04:23:17.308+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-cos-3.3.2.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.apache.hadoop_hadoop-cos-3.3.2.jar
[2025-06-03T04:23:17.310+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar at spark://083f94398a74:38887/files/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.311+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.aliyun.oss_aliyun-sdk-oss-3.13.0.jar
[2025-06-03T04:23:17.313+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.jdom_jdom2-2.0.6.jar at spark://083f94398a74:38887/files/org.jdom_jdom2-2.0.6.jar with timestamp 1748924596513
[2025-06-03T04:23:17.313+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.jdom_jdom2-2.0.6.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.jdom_jdom2-2.0.6.jar
[2025-06-03T04:23:17.315+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar at spark://083f94398a74:38887/files/org.codehaus.jettison_jettison-1.1.jar with timestamp 1748924596513
[2025-06-03T04:23:17.316+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.codehaus.jettison_jettison-1.1.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.codehaus.jettison_jettison-1.1.jar
[2025-06-03T04:23:17.318+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-core-4.5.10.jar at spark://083f94398a74:38887/files/com.aliyun_aliyun-java-sdk-core-4.5.10.jar with timestamp 1748924596513
[2025-06-03T04:23:17.318+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-core-4.5.10.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.aliyun_aliyun-java-sdk-core-4.5.10.jar
[2025-06-03T04:23:17.320+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar at spark://083f94398a74:38887/files/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.320+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.aliyun_aliyun-java-sdk-ram-3.1.0.jar
[2025-06-03T04:23:17.323+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar at spark://083f94398a74:38887/files/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.323+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.aliyun_aliyun-java-sdk-kms-2.11.0.jar
[2025-06-03T04:23:17.325+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/stax_stax-api-1.0.1.jar at spark://083f94398a74:38887/files/stax_stax-api-1.0.1.jar with timestamp 1748924596513
[2025-06-03T04:23:17.325+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/stax_stax-api-1.0.1.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/stax_stax-api-1.0.1.jar
[2025-06-03T04:23:17.327+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.gson_gson-2.8.9.jar at spark://083f94398a74:38887/files/com.google.code.gson_gson-2.8.9.jar with timestamp 1748924596513
[2025-06-03T04:23:17.328+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.gson_gson-2.8.9.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.google.code.gson_gson-2.8.9.jar
[2025-06-03T04:23:17.330+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar at spark://083f94398a74:38887/files/javax.xml.bind_jaxb-api-2.2.11.jar with timestamp 1748924596513
[2025-06-03T04:23:17.330+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/javax.xml.bind_jaxb-api-2.2.11.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/javax.xml.bind_jaxb-api-2.2.11.jar
[2025-06-03T04:23:17.332+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.ini4j_ini4j-0.5.4.jar at spark://083f94398a74:38887/files/org.ini4j_ini4j-0.5.4.jar with timestamp 1748924596513
[2025-06-03T04:23:17.332+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.ini4j_ini4j-0.5.4.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.ini4j_ini4j-0.5.4.jar
[2025-06-03T04:23:17.334+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.opentracing_opentracing-api-0.33.0.jar at spark://083f94398a74:38887/files/io.opentracing_opentracing-api-0.33.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.335+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/io.opentracing_opentracing-api-0.33.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/io.opentracing_opentracing-api-0.33.0.jar
[2025-06-03T04:23:17.337+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.opentracing_opentracing-util-0.33.0.jar at spark://083f94398a74:38887/files/io.opentracing_opentracing-util-0.33.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.337+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/io.opentracing_opentracing-util-0.33.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/io.opentracing_opentracing-util-0.33.0.jar
[2025-06-03T04:23:17.340+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.opentracing_opentracing-noop-0.33.0.jar at spark://083f94398a74:38887/files/io.opentracing_opentracing-noop-0.33.0.jar with timestamp 1748924596513
[2025-06-03T04:23:17.340+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/io.opentracing_opentracing-noop-0.33.0.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/io.opentracing_opentracing-noop-0.33.0.jar
[2025-06-03T04:23:17.343+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar at spark://083f94398a74:38887/files/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar with timestamp 1748924596513
[2025-06-03T04:23:17.343+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.microsoft.azure_azure-data-lake-store-sdk-2.3.9.jar
[2025-06-03T04:23:17.346+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.qcloud_cos_api-bundle-5.6.19.jar at spark://083f94398a74:38887/files/com.qcloud_cos_api-bundle-5.6.19.jar with timestamp 1748924596513
[2025-06-03T04:23:17.346+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/com.qcloud_cos_api-bundle-5.6.19.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/com.qcloud_cos_api-bundle-5.6.19.jar
[2025-06-03T04:23:17.354+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar at spark://083f94398a74:38887/files/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar with timestamp 1748924596513
[2025-06-03T04:23:17.354+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Copying /home/***/.ivy2/jars/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar to /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/userFiles-db865ac0-e336-4b1d-9a7b-1f0a1b71542a/org.eclipse.jetty_jetty-util-9.4.46.v20220331.jar
[2025-06-03T04:23:17.540+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-06-03T04:23:17.569+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.6:7077 after 16 ms (0 ms spent in bootstraps)
[2025-06-03T04:23:17.622+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250603042317-0021
[2025-06-03T04:23:17.624+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250603042317-0021/0 on worker-20250603030932-172.18.0.7-43297 (172.18.0.7:43297) with 4 core(s)
[2025-06-03T04:23:17.624+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO StandaloneSchedulerBackend: Granted executor ID app-20250603042317-0021/0 on hostPort 172.18.0.7:43297 with 4 core(s), 2.0 GiB RAM
[2025-06-03T04:23:17.628+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34473.
[2025-06-03T04:23:17.628+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO NettyBlockTransferService: Server created on 083f94398a74:34473
[2025-06-03T04:23:17.629+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-06-03T04:23:17.633+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 083f94398a74, 34473, None)
[2025-06-03T04:23:17.635+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO BlockManagerMasterEndpoint: Registering block manager 083f94398a74:34473 with 434.4 MiB RAM, BlockManagerId(driver, 083f94398a74, 34473, None)
[2025-06-03T04:23:17.637+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 083f94398a74, 34473, None)
[2025-06-03T04:23:17.637+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 083f94398a74, 34473, None)
[2025-06-03T04:23:17.659+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250603042317-0021/0 is now RUNNING
[2025-06-03T04:23:17.765+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-06-03T04:23:17.950+0000] {spark_submit.py:644} INFO - Bronze Ingestion: Spark session created.
[2025-06-03T04:23:17.955+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-06-03T04:23:18.040+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:18 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-06-03T04:23:18.047+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:18 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-06-03T04:23:18.047+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:18 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-06-03T04:23:18.384+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:18 INFO SharedState: Warehouse path is 's3a://mybucket'.
[2025-06-03T04:23:19.043+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:19 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.
[2025-06-03T04:23:19.286+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:19 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:43684) with ID 0,  ResourceProfileId 0
[2025-06-03T04:23:19.329+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:44331 with 1048.8 MiB RAM, BlockManagerId(0, 172.18.0.7, 44331, None)
[2025-06-03T04:23:19.582+0000] {spark_submit.py:644} INFO - Successfully loaded Stephen Curry stats from: /opt/***/data/Stephen Curry Stats.parquet
[2025-06-03T04:23:19.585+0000] {spark_submit.py:644} INFO - ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3
[2025-06-03T04:23:19.586+0000] {spark_submit.py:644} INFO - ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3
[2025-06-03T04:23:19.590+0000] {spark_submit.py:644} INFO - ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3
[2025-06-03T04:23:19.590+0000] {spark_submit.py:644} INFO - ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3
[2025-06-03T04:23:20.427+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:20 INFO HiveConf: Found configuration file null
[2025-06-03T04:23:20.435+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:20 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
[2025-06-03T04:23:20.573+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:20 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is s3a://mybucket
[2025-06-03T04:23:20.609+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:20 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
[2025-06-03T04:23:20.637+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:20 INFO metastore: Opened a connection to metastore, current connections: 1
[2025-06-03T04:23:20.665+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:20 INFO metastore: Connected to metastore.
[2025-06-03T04:23:20.837+0000] {spark_submit.py:644} INFO - Writing data to Bronze layer at: s3a://mybucket/bronze/curry_raw_data/
[2025-06-03T04:23:21.035+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:21 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2025-06-03T04:23:22.024+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:22 INFO DeltaLog: Loading version 0.
[2025-06-03T04:23:22.053+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:22 INFO Snapshot: [tableId=4eecea56-cae7-4bba-9470-46cdd5c92594] DELTA: Compute snapshot for version: 0
[2025-06-03T04:23:22.197+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 226.0 KiB, free 434.2 MiB)
[2025-06-03T04:23:22.235+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 434.1 MiB)
[2025-06-03T04:23:22.237+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 083f94398a74:34473 (size: 41.8 KiB, free: 434.4 MiB)
[2025-06-03T04:23:22.240+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:22 INFO SparkContext: Created broadcast 0 from toString at String.java:4220
[2025-06-03T04:23:22.266+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:22 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 3905)
[2025-06-03T04:23:22.754+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:22 INFO FileSourceStrategy: Pushed Filters:
[2025-06-03T04:23:22.754+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:22 INFO FileSourceStrategy: Post-Scan Filters:
[2025-06-03T04:23:23.138+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO CodeGenerator: Code generated in 224.923583 ms
[2025-06-03T04:23:23.140+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 226.3 KiB, free 433.9 MiB)
[2025-06-03T04:23:23.147+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 433.9 MiB)
[2025-06-03T04:23:23.149+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 083f94398a74:34473 (size: 41.8 KiB, free: 434.3 MiB)
[2025-06-03T04:23:23.151+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO SparkContext: Created broadcast 1 from toString at String.java:4220
[2025-06-03T04:23:23.264+0000] {spark_submit.py:644} INFO - Error writing data to Bronze layer: An error occurred while calling o68.saveAsTable.
[2025-06-03T04:23:23.265+0000] {spark_submit.py:644} INFO - : java.lang.ClassCastException: class org.apache.hadoop.fs.s3a.S3AFileStatus cannot be cast to class org.apache.spark.sql.execution.datasources.FileStatusWithMetadata (org.apache.hadoop.fs.s3a.S3AFileStatus is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @35563e4c; org.apache.spark.sql.execution.datasources.FileStatusWithMetadata is in unnamed module of loader 'app')
[2025-06-03T04:23:23.265+0000] {spark_submit.py:644} INFO - at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-06-03T04:23:23.265+0000] {spark_submit.py:644} INFO - at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
[2025-06-03T04:23:23.266+0000] {spark_submit.py:644} INFO - at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
[2025-06-03T04:23:23.266+0000] {spark_submit.py:644} INFO - at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
[2025-06-03T04:23:23.266+0000] {spark_submit.py:644} INFO - at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-06-03T04:23:23.266+0000] {spark_submit.py:644} INFO - at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-06-03T04:23:23.267+0000] {spark_submit.py:644} INFO - at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-06-03T04:23:23.267+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2(DataSourceScanExec.scala:466)
[2025-06-03T04:23:23.267+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2$adapted(DataSourceScanExec.scala:466)
[2025-06-03T04:23:23.267+0000] {spark_submit.py:644} INFO - at scala.collection.immutable.List.map(List.scala:293)
[2025-06-03T04:23:23.267+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.setFilesNumAndSizeMetric(DataSourceScanExec.scala:466)
[2025-06-03T04:23:23.268+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions(DataSourceScanExec.scala:257)
[2025-06-03T04:23:23.268+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions$(DataSourceScanExec.scala:251)
[2025-06-03T04:23:23.268+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions$lzycompute(DataSourceScanExec.scala:506)
[2025-06-03T04:23:23.268+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions(DataSourceScanExec.scala:506)
[2025-06-03T04:23:23.268+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions(DataSourceScanExec.scala:286)
[2025-06-03T04:23:23.268+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions$(DataSourceScanExec.scala:267)
[2025-06-03T04:23:23.269+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions$lzycompute(DataSourceScanExec.scala:506)
[2025-06-03T04:23:23.269+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions(DataSourceScanExec.scala:506)
[2025-06-03T04:23:23.269+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:553)
[2025-06-03T04:23:23.269+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
[2025-06-03T04:23:23.269+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
[2025-06-03T04:23:23.269+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
[2025-06-03T04:23:23.270+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
[2025-06-03T04:23:23.270+0000] {spark_submit.py:644} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-06-03T04:23:23.270+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
[2025-06-03T04:23:23.270+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
[2025-06-03T04:23:23.270+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
[2025-06-03T04:23:23.271+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
[2025-06-03T04:23:23.271+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
[2025-06-03T04:23:23.271+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
[2025-06-03T04:23:23.271+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
[2025-06-03T04:23:23.271+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
[2025-06-03T04:23:23.272+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
[2025-06-03T04:23:23.272+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
[2025-06-03T04:23:23.272+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
[2025-06-03T04:23:23.273+0000] {spark_submit.py:644} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-06-03T04:23:23.273+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
[2025-06-03T04:23:23.273+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
[2025-06-03T04:23:23.273+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:141)
[2025-06-03T04:23:23.274+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:141)
[2025-06-03T04:23:23.274+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:146)
[2025-06-03T04:23:23.274+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)
[2025-06-03T04:23:23.274+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)
[2025-06-03T04:23:23.274+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
[2025-06-03T04:23:23.275+0000] {spark_submit.py:644} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-06-03T04:23:23.275+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
[2025-06-03T04:23:23.275+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)
[2025-06-03T04:23:23.275+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)
[2025-06-03T04:23:23.276+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)
[2025-06-03T04:23:23.276+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:187)
[2025-06-03T04:23:23.276+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:187)
[2025-06-03T04:23:23.276+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:189)
[2025-06-03T04:23:23.276+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)
[2025-06-03T04:23:23.276+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:302)
[2025-06-03T04:23:23.277+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:300)
[2025-06-03T04:23:23.277+0000] {spark_submit.py:644} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-06-03T04:23:23.277+0000] {spark_submit.py:644} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-06-03T04:23:23.277+0000] {spark_submit.py:644} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-06-03T04:23:23.277+0000] {spark_submit.py:644} INFO - at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-06-03T04:23:23.277+0000] {spark_submit.py:644} INFO - at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-06-03T04:23:23.277+0000] {spark_submit.py:644} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-06-03T04:23:23.278+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:300)
[2025-06-03T04:23:23.278+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-06-03T04:23:23.278+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)
[2025-06-03T04:23:23.278+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)
[2025-06-03T04:23:23.278+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:402)
[2025-06-03T04:23:23.278+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
[2025-06-03T04:23:23.279+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
[2025-06-03T04:23:23.279+0000] {spark_submit.py:644} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-06-03T04:23:23.279+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
[2025-06-03T04:23:23.279+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
[2025-06-03T04:23:23.279+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
[2025-06-03T04:23:23.279+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
[2025-06-03T04:23:23.280+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache$CachedDS.$anonfun$cachedDs$2(StateCache.scala:61)
[2025-06-03T04:23:23.280+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.280+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.280+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.280+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache$CachedDS.$anonfun$cachedDs$1(StateCache.scala:61)
[2025-06-03T04:23:23.280+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-06-03T04:23:23.281+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-06-03T04:23:23.281+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-06-03T04:23:23.281+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-06-03T04:23:23.281+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-06-03T04:23:23.281+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache$CachedDS.<init>(StateCache.scala:58)
[2025-06-03T04:23:23.282+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache.$anonfun$cacheDS$1(StateCache.scala:110)
[2025-06-03T04:23:23.282+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.282+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.282+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.282+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache.cacheDS(StateCache.scala:110)
[2025-06-03T04:23:23.282+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache.cacheDS$(StateCache.scala:108)
[2025-06-03T04:23:23.282+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.cacheDS(Snapshot.scala:66)
[2025-06-03T04:23:23.283+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$cachedState$1(Snapshot.scala:164)
[2025-06-03T04:23:23.283+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.283+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.283+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.284+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.cachedState$lzycompute(Snapshot.scala:162)
[2025-06-03T04:23:23.284+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.cachedState(Snapshot.scala:162)
[2025-06-03T04:23:23.284+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$stateDF$1(Snapshot.scala:174)
[2025-06-03T04:23:23.284+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.284+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.285+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.285+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.stateDF(Snapshot.scala:174)
[2025-06-03T04:23:23.285+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$4(Snapshot.scala:207)
[2025-06-03T04:23:23.285+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.286+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.286+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.286+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$2(Snapshot.scala:207)
[2025-06-03T04:23:23.287+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.287+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.288+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.288+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$1(Snapshot.scala:202)
[2025-06-03T04:23:23.289+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)
[2025-06-03T04:23:23.289+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)
[2025-06-03T04:23:23.290+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)
[2025-06-03T04:23:23.290+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.withStatusCode(Snapshot.scala:66)
[2025-06-03T04:23:23.290+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.computedState$lzycompute(Snapshot.scala:202)
[2025-06-03T04:23:23.291+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.computedState(Snapshot.scala:200)
[2025-06-03T04:23:23.291+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:238)
[2025-06-03T04:23:23.291+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.toString(Snapshot.scala:364)
[2025-06-03T04:23:23.291+0000] {spark_submit.py:644} INFO - at java.base/java.lang.String.valueOf(String.java:4220)
[2025-06-03T04:23:23.292+0000] {spark_submit.py:644} INFO - at java.base/java.lang.StringBuilder.append(StringBuilder.java:173)
[2025-06-03T04:23:23.292+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$new$1(Snapshot.scala:367)
[2025-06-03T04:23:23.292+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$logInfo$1(Snapshot.scala:344)
[2025-06-03T04:23:23.292+0000] {spark_submit.py:644} INFO - at org.apache.spark.internal.Logging.logInfo(Logging.scala:60)
[2025-06-03T04:23:23.293+0000] {spark_submit.py:644} INFO - at org.apache.spark.internal.Logging.logInfo$(Logging.scala:59)
[2025-06-03T04:23:23.293+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.logInfo(Snapshot.scala:344)
[2025-06-03T04:23:23.293+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:367)
[2025-06-03T04:23:23.293+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$4(SnapshotManagement.scala:327)
[2025-06-03T04:23:23.294+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:472)
[2025-06-03T04:23:23.294+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:460)
[2025-06-03T04:23:23.294+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:67)
[2025-06-03T04:23:23.294+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:317)
[2025-06-03T04:23:23.294+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:309)
[2025-06-03T04:23:23.295+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:67)
[2025-06-03T04:23:23.295+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshotAtInitInternal$1(SnapshotManagement.scala:278)
[2025-06-03T04:23:23.295+0000] {spark_submit.py:644} INFO - at scala.Option.map(Option.scala:230)
[2025-06-03T04:23:23.295+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotAtInitInternal(SnapshotManagement.scala:273)
[2025-06-03T04:23:23.295+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotAtInitInternal$(SnapshotManagement.scala:269)
[2025-06-03T04:23:23.295+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.createSnapshotAtInitInternal(DeltaLog.scala:67)
[2025-06-03T04:23:23.295+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:264)
[2025-06-03T04:23:23.295+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.296+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.296+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:67)
[2025-06-03T04:23:23.296+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:258)
[2025-06-03T04:23:23.296+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:257)
[2025-06-03T04:23:23.297+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:67)
[2025-06-03T04:23:23.297+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:55)
[2025-06-03T04:23:23.297+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:72)
[2025-06-03T04:23:23.297+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:737)
[2025-06-03T04:23:23.297+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
[2025-06-03T04:23:23.297+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:733)
[2025-06-03T04:23:23.297+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.297+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.298+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:550)
[2025-06-03T04:23:23.298+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)
[2025-06-03T04:23:23.298+0000] {spark_submit.py:644} INFO - at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
[2025-06-03T04:23:23.298+0000] {spark_submit.py:644} INFO - at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
[2025-06-03T04:23:23.298+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:550)
[2025-06-03T04:23:23.299+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)
[2025-06-03T04:23:23.299+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)
[2025-06-03T04:23:23.299+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)
[2025-06-03T04:23:23.299+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:550)
[2025-06-03T04:23:23.299+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:732)
[2025-06-03T04:23:23.300+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:748)
[2025-06-03T04:23:23.300+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)
[2025-06-03T04:23:23.300+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
[2025-06-03T04:23:23.300+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
[2025-06-03T04:23:23.301+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
[2025-06-03T04:23:23.301+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
[2025-06-03T04:23:23.301+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
[2025-06-03T04:23:23.301+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
[2025-06-03T04:23:23.302+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:748)
[2025-06-03T04:23:23.302+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:755)
[2025-06-03T04:23:23.302+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:613)
[2025-06-03T04:23:23.302+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:84)
[2025-06-03T04:23:23.302+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:83)
[2025-06-03T04:23:23.302+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:115)
[2025-06-03T04:23:23.302+0000] {spark_submit.py:644} INFO - at scala.Option.getOrElse(Option.scala:189)
[2025-06-03T04:23:23.302+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:115)
[2025-06-03T04:23:23.302+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:102)
[2025-06-03T04:23:23.303+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:121)
[2025-06-03T04:23:23.303+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:119)
[2025-06-03T04:23:23.303+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.schema(DeltaTableV2.scala:123)
[2025-06-03T04:23:23.303+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.connector.catalog.Table.columns(Table.java:65)
[2025-06-03T04:23:23.303+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$.create(DataSourceV2Relation.scala:197)
[2025-06-03T04:23:23.303+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$.create(DataSourceV2Relation.scala:205)
[2025-06-03T04:23:23.303+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.invalidateCache(DataSourceV2Strategy.scala:96)
[2025-06-03T04:23:23.304+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$6(DataSourceV2Strategy.scala:232)
[2025-06-03T04:23:23.304+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$6$adapted(DataSourceV2Strategy.scala:232)
[2025-06-03T04:23:23.304+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:200)
[2025-06-03T04:23:23.304+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-06-03T04:23:23.304+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-06-03T04:23:23.304+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-06-03T04:23:23.304+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
[2025-06-03T04:23:23.304+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-06-03T04:23:23.304+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-06-03T04:23:23.304+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-06-03T04:23:23.305+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-06-03T04:23:23.305+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-06-03T04:23:23.305+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
[2025-06-03T04:23:23.305+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-06-03T04:23:23.305+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
[2025-06-03T04:23:23.305+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
[2025-06-03T04:23:23.305+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
[2025-06-03T04:23:23.305+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
[2025-06-03T04:23:23.305+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-06-03T04:23:23.306+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-06-03T04:23:23.306+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2025-06-03T04:23:23.306+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2025-06-03T04:23:23.306+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
[2025-06-03T04:23:23.306+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
[2025-06-03T04:23:23.306+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
[2025-06-03T04:23:23.306+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
[2025-06-03T04:23:23.306+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
[2025-06-03T04:23:23.306+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
[2025-06-03T04:23:23.307+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:634)
[2025-06-03T04:23:23.307+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
[2025-06-03T04:23:23.307+0000] {spark_submit.py:644} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-06-03T04:23:23.307+0000] {spark_submit.py:644} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-06-03T04:23:23.307+0000] {spark_submit.py:644} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-06-03T04:23:23.307+0000] {spark_submit.py:644} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-06-03T04:23:23.308+0000] {spark_submit.py:644} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-06-03T04:23:23.308+0000] {spark_submit.py:644} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-06-03T04:23:23.308+0000] {spark_submit.py:644} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-06-03T04:23:23.308+0000] {spark_submit.py:644} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-06-03T04:23:23.309+0000] {spark_submit.py:644} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-06-03T04:23:23.309+0000] {spark_submit.py:644} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-06-03T04:23:23.309+0000] {spark_submit.py:644} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-06-03T04:23:23.309+0000] {spark_submit.py:644} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-06-03T04:23:23.309+0000] {spark_submit.py:644} INFO - 
[2025-06-03T04:23:23.309+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-06-03T04:23:23.309+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO SparkUI: Stopped Spark web UI at http://083f94398a74:4040
[2025-06-03T04:23:23.309+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-06-03T04:23:23.309+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-06-03T04:23:23.309+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-06-03T04:23:23.310+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO MemoryStore: MemoryStore cleared
[2025-06-03T04:23:23.311+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO BlockManager: BlockManager stopped
[2025-06-03T04:23:23.313+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-06-03T04:23:23.315+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-06-03T04:23:23.327+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO SparkContext: Successfully stopped SparkContext
[2025-06-03T04:23:23.811+0000] {spark_submit.py:644} INFO - Traceback (most recent call last):
[2025-06-03T04:23:23.812+0000] {spark_submit.py:644} INFO - File "/opt/***/dags/bronze_ingestion_sc_stats_etl.py", line 88, in <module>
[2025-06-03T04:23:23.812+0000] {spark_submit.py:644} INFO - main(args.output_path)
[2025-06-03T04:23:23.813+0000] {spark_submit.py:644} INFO - File "/opt/***/dags/bronze_ingestion_sc_stats_etl.py", line 72, in main
[2025-06-03T04:23:23.814+0000] {spark_submit.py:644} INFO - .saveAsTable("sc_stats_db.bronze_sc_stats"))
[2025-06-03T04:23:23.814+0000] {spark_submit.py:644} INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-06-03T04:23:23.815+0000] {spark_submit.py:644} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1586, in saveAsTable
[2025-06-03T04:23:23.815+0000] {spark_submit.py:644} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-06-03T04:23:23.816+0000] {spark_submit.py:644} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
[2025-06-03T04:23:23.816+0000] {spark_submit.py:644} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-06-03T04:23:23.859+0000] {spark_submit.py:644} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o68.saveAsTable.
[2025-06-03T04:23:23.859+0000] {spark_submit.py:644} INFO - : java.lang.ClassCastException: class org.apache.hadoop.fs.s3a.S3AFileStatus cannot be cast to class org.apache.spark.sql.execution.datasources.FileStatusWithMetadata (org.apache.hadoop.fs.s3a.S3AFileStatus is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @35563e4c; org.apache.spark.sql.execution.datasources.FileStatusWithMetadata is in unnamed module of loader 'app')
[2025-06-03T04:23:23.860+0000] {spark_submit.py:644} INFO - at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-06-03T04:23:23.860+0000] {spark_submit.py:644} INFO - at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
[2025-06-03T04:23:23.860+0000] {spark_submit.py:644} INFO - at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
[2025-06-03T04:23:23.861+0000] {spark_submit.py:644} INFO - at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
[2025-06-03T04:23:23.861+0000] {spark_submit.py:644} INFO - at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-06-03T04:23:23.861+0000] {spark_submit.py:644} INFO - at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-06-03T04:23:23.861+0000] {spark_submit.py:644} INFO - at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-06-03T04:23:23.861+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2(DataSourceScanExec.scala:466)
[2025-06-03T04:23:23.862+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.$anonfun$setFilesNumAndSizeMetric$2$adapted(DataSourceScanExec.scala:466)
[2025-06-03T04:23:23.862+0000] {spark_submit.py:644} INFO - at scala.collection.immutable.List.map(List.scala:293)
[2025-06-03T04:23:23.862+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.setFilesNumAndSizeMetric(DataSourceScanExec.scala:466)
[2025-06-03T04:23:23.862+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions(DataSourceScanExec.scala:257)
[2025-06-03T04:23:23.863+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.selectedPartitions$(DataSourceScanExec.scala:251)
[2025-06-03T04:23:23.863+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions$lzycompute(DataSourceScanExec.scala:506)
[2025-06-03T04:23:23.863+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions(DataSourceScanExec.scala:506)
[2025-06-03T04:23:23.863+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions(DataSourceScanExec.scala:286)
[2025-06-03T04:23:23.863+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanLike.dynamicallySelectedPartitions$(DataSourceScanExec.scala:267)
[2025-06-03T04:23:23.864+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions$lzycompute(DataSourceScanExec.scala:506)
[2025-06-03T04:23:23.864+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions(DataSourceScanExec.scala:506)
[2025-06-03T04:23:23.864+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:553)
[2025-06-03T04:23:23.864+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)
[2025-06-03T04:23:23.864+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)
[2025-06-03T04:23:23.865+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
[2025-06-03T04:23:23.865+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
[2025-06-03T04:23:23.865+0000] {spark_submit.py:644} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-06-03T04:23:23.865+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
[2025-06-03T04:23:23.865+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
[2025-06-03T04:23:23.865+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
[2025-06-03T04:23:23.866+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
[2025-06-03T04:23:23.866+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
[2025-06-03T04:23:23.866+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
[2025-06-03T04:23:23.866+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
[2025-06-03T04:23:23.866+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)
[2025-06-03T04:23:23.866+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
[2025-06-03T04:23:23.867+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
[2025-06-03T04:23:23.867+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
[2025-06-03T04:23:23.867+0000] {spark_submit.py:644} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-06-03T04:23:23.867+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
[2025-06-03T04:23:23.867+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
[2025-06-03T04:23:23.867+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:141)
[2025-06-03T04:23:23.868+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:141)
[2025-06-03T04:23:23.868+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:146)
[2025-06-03T04:23:23.868+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:145)
[2025-06-03T04:23:23.868+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:73)
[2025-06-03T04:23:23.868+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
[2025-06-03T04:23:23.868+0000] {spark_submit.py:644} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-06-03T04:23:23.868+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
[2025-06-03T04:23:23.868+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:73)
[2025-06-03T04:23:23.869+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:72)
[2025-06-03T04:23:23.869+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:120)
[2025-06-03T04:23:23.869+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:187)
[2025-06-03T04:23:23.869+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:187)
[2025-06-03T04:23:23.869+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:189)
[2025-06-03T04:23:23.869+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)
[2025-06-03T04:23:23.869+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:302)
[2025-06-03T04:23:23.869+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:300)
[2025-06-03T04:23:23.869+0000] {spark_submit.py:644} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-06-03T04:23:23.870+0000] {spark_submit.py:644} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-06-03T04:23:23.870+0000] {spark_submit.py:644} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-06-03T04:23:23.870+0000] {spark_submit.py:644} INFO - at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-06-03T04:23:23.870+0000] {spark_submit.py:644} INFO - at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-06-03T04:23:23.870+0000] {spark_submit.py:644} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-06-03T04:23:23.871+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:300)
[2025-06-03T04:23:23.871+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-06-03T04:23:23.871+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)
[2025-06-03T04:23:23.871+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)
[2025-06-03T04:23:23.871+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:402)
[2025-06-03T04:23:23.871+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
[2025-06-03T04:23:23.872+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
[2025-06-03T04:23:23.872+0000] {spark_submit.py:644} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-06-03T04:23:23.872+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
[2025-06-03T04:23:23.872+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
[2025-06-03T04:23:23.872+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
[2025-06-03T04:23:23.873+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
[2025-06-03T04:23:23.873+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache$CachedDS.$anonfun$cachedDs$2(StateCache.scala:61)
[2025-06-03T04:23:23.873+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.873+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.873+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.874+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache$CachedDS.$anonfun$cachedDs$1(StateCache.scala:61)
[2025-06-03T04:23:23.874+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-06-03T04:23:23.874+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-06-03T04:23:23.874+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-06-03T04:23:23.874+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-06-03T04:23:23.875+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-06-03T04:23:23.875+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache$CachedDS.<init>(StateCache.scala:58)
[2025-06-03T04:23:23.875+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache.$anonfun$cacheDS$1(StateCache.scala:110)
[2025-06-03T04:23:23.875+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.875+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.875+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.875+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache.cacheDS(StateCache.scala:110)
[2025-06-03T04:23:23.876+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.StateCache.cacheDS$(StateCache.scala:108)
[2025-06-03T04:23:23.876+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.cacheDS(Snapshot.scala:66)
[2025-06-03T04:23:23.876+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$cachedState$1(Snapshot.scala:164)
[2025-06-03T04:23:23.876+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.876+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.876+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.876+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.cachedState$lzycompute(Snapshot.scala:162)
[2025-06-03T04:23:23.877+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.cachedState(Snapshot.scala:162)
[2025-06-03T04:23:23.877+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$stateDF$1(Snapshot.scala:174)
[2025-06-03T04:23:23.877+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.877+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.877+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.877+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.stateDF(Snapshot.scala:174)
[2025-06-03T04:23:23.877+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$4(Snapshot.scala:207)
[2025-06-03T04:23:23.877+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.878+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.878+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.878+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$2(Snapshot.scala:207)
[2025-06-03T04:23:23.878+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.878+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.878+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:66)
[2025-06-03T04:23:23.878+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$1(Snapshot.scala:202)
[2025-06-03T04:23:23.878+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)
[2025-06-03T04:23:23.879+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)
[2025-06-03T04:23:23.879+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)
[2025-06-03T04:23:23.879+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.withStatusCode(Snapshot.scala:66)
[2025-06-03T04:23:23.879+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.computedState$lzycompute(Snapshot.scala:202)
[2025-06-03T04:23:23.879+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.computedState(Snapshot.scala:200)
[2025-06-03T04:23:23.879+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:238)
[2025-06-03T04:23:23.879+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.toString(Snapshot.scala:364)
[2025-06-03T04:23:23.880+0000] {spark_submit.py:644} INFO - at java.base/java.lang.String.valueOf(String.java:4220)
[2025-06-03T04:23:23.880+0000] {spark_submit.py:644} INFO - at java.base/java.lang.StringBuilder.append(StringBuilder.java:173)
[2025-06-03T04:23:23.880+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$new$1(Snapshot.scala:367)
[2025-06-03T04:23:23.880+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.$anonfun$logInfo$1(Snapshot.scala:344)
[2025-06-03T04:23:23.880+0000] {spark_submit.py:644} INFO - at org.apache.spark.internal.Logging.logInfo(Logging.scala:60)
[2025-06-03T04:23:23.880+0000] {spark_submit.py:644} INFO - at org.apache.spark.internal.Logging.logInfo$(Logging.scala:59)
[2025-06-03T04:23:23.880+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.logInfo(Snapshot.scala:344)
[2025-06-03T04:23:23.880+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:367)
[2025-06-03T04:23:23.881+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$4(SnapshotManagement.scala:327)
[2025-06-03T04:23:23.881+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:472)
[2025-06-03T04:23:23.881+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:460)
[2025-06-03T04:23:23.881+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:67)
[2025-06-03T04:23:23.882+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:317)
[2025-06-03T04:23:23.882+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:309)
[2025-06-03T04:23:23.882+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:67)
[2025-06-03T04:23:23.882+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshotAtInitInternal$1(SnapshotManagement.scala:278)
[2025-06-03T04:23:23.882+0000] {spark_submit.py:644} INFO - at scala.Option.map(Option.scala:230)
[2025-06-03T04:23:23.882+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotAtInitInternal(SnapshotManagement.scala:273)
[2025-06-03T04:23:23.883+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotAtInitInternal$(SnapshotManagement.scala:269)
[2025-06-03T04:23:23.883+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.createSnapshotAtInitInternal(DeltaLog.scala:67)
[2025-06-03T04:23:23.883+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:264)
[2025-06-03T04:23:23.883+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.883+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.883+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:67)
[2025-06-03T04:23:23.883+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:258)
[2025-06-03T04:23:23.883+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:257)
[2025-06-03T04:23:23.884+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:67)
[2025-06-03T04:23:23.884+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:55)
[2025-06-03T04:23:23.884+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:72)
[2025-06-03T04:23:23.884+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:737)
[2025-06-03T04:23:23.884+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
[2025-06-03T04:23:23.884+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:733)
[2025-06-03T04:23:23.884+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2025-06-03T04:23:23.884+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2025-06-03T04:23:23.884+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:550)
[2025-06-03T04:23:23.885+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)
[2025-06-03T04:23:23.885+0000] {spark_submit.py:644} INFO - at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
[2025-06-03T04:23:23.885+0000] {spark_submit.py:644} INFO - at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
[2025-06-03T04:23:23.885+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:550)
[2025-06-03T04:23:23.885+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)
[2025-06-03T04:23:23.885+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)
[2025-06-03T04:23:23.885+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)
[2025-06-03T04:23:23.885+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:550)
[2025-06-03T04:23:23.885+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:732)
[2025-06-03T04:23:23.885+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:748)
[2025-06-03T04:23:23.886+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)
[2025-06-03T04:23:23.886+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
[2025-06-03T04:23:23.886+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
[2025-06-03T04:23:23.886+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
[2025-06-03T04:23:23.886+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
[2025-06-03T04:23:23.886+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
[2025-06-03T04:23:23.886+0000] {spark_submit.py:644} INFO - at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
[2025-06-03T04:23:23.886+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:748)
[2025-06-03T04:23:23.886+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:755)
[2025-06-03T04:23:23.887+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:613)
[2025-06-03T04:23:23.887+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:84)
[2025-06-03T04:23:23.887+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:83)
[2025-06-03T04:23:23.887+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:115)
[2025-06-03T04:23:23.887+0000] {spark_submit.py:644} INFO - at scala.Option.getOrElse(Option.scala:189)
[2025-06-03T04:23:23.887+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:115)
[2025-06-03T04:23:23.887+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:102)
[2025-06-03T04:23:23.887+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:121)
[2025-06-03T04:23:23.887+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:119)
[2025-06-03T04:23:23.888+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.delta.catalog.DeltaTableV2.schema(DeltaTableV2.scala:123)
[2025-06-03T04:23:23.888+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.connector.catalog.Table.columns(Table.java:65)
[2025-06-03T04:23:23.888+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$.create(DataSourceV2Relation.scala:197)
[2025-06-03T04:23:23.888+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$.create(DataSourceV2Relation.scala:205)
[2025-06-03T04:23:23.888+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.invalidateCache(DataSourceV2Strategy.scala:96)
[2025-06-03T04:23:23.888+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$6(DataSourceV2Strategy.scala:232)
[2025-06-03T04:23:23.888+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.$anonfun$apply$6$adapted(DataSourceV2Strategy.scala:232)
[2025-06-03T04:23:23.888+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:200)
[2025-06-03T04:23:23.888+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-06-03T04:23:23.889+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-06-03T04:23:23.889+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-06-03T04:23:23.889+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
[2025-06-03T04:23:23.889+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-06-03T04:23:23.889+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-06-03T04:23:23.889+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-06-03T04:23:23.889+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-06-03T04:23:23.889+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-06-03T04:23:23.890+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
[2025-06-03T04:23:23.890+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-06-03T04:23:23.890+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
[2025-06-03T04:23:23.890+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
[2025-06-03T04:23:23.890+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
[2025-06-03T04:23:23.890+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
[2025-06-03T04:23:23.890+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-06-03T04:23:23.890+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-06-03T04:23:23.890+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2025-06-03T04:23:23.891+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2025-06-03T04:23:23.891+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
[2025-06-03T04:23:23.891+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
[2025-06-03T04:23:23.891+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
[2025-06-03T04:23:23.891+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
[2025-06-03T04:23:23.891+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
[2025-06-03T04:23:23.891+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
[2025-06-03T04:23:23.891+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:634)
[2025-06-03T04:23:23.891+0000] {spark_submit.py:644} INFO - at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
[2025-06-03T04:23:23.892+0000] {spark_submit.py:644} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-06-03T04:23:23.892+0000] {spark_submit.py:644} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-06-03T04:23:23.892+0000] {spark_submit.py:644} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-06-03T04:23:23.892+0000] {spark_submit.py:644} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-06-03T04:23:23.892+0000] {spark_submit.py:644} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-06-03T04:23:23.892+0000] {spark_submit.py:644} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-06-03T04:23:23.892+0000] {spark_submit.py:644} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-06-03T04:23:23.892+0000] {spark_submit.py:644} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-06-03T04:23:23.892+0000] {spark_submit.py:644} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-06-03T04:23:23.893+0000] {spark_submit.py:644} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-06-03T04:23:23.893+0000] {spark_submit.py:644} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-06-03T04:23:23.893+0000] {spark_submit.py:644} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-06-03T04:23:23.893+0000] {spark_submit.py:644} INFO - 
[2025-06-03T04:23:23.899+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO ShutdownHookManager: Shutdown hook called
[2025-06-03T04:23:23.899+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331/pyspark-87af9034-c514-45d9-85b3-e42c648430f8
[2025-06-03T04:23:23.901+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a4e1c48-27b8-496a-a41e-64587907c331
[2025-06-03T04:23:23.903+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-7f7c8f27-2fcd-402f-ba77-223a3aa0fde8
[2025-06-03T04:23:23.906+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-06-03T04:23:23.906+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-06-03T04:23:23.907+0000] {spark_submit.py:644} INFO - 25/06/03 04:23:23 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-06-03T04:23:23.950+0000] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 197, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 566, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.executor.memory=2g --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minioadmin --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.sql.warehouse.dir=s3a://mybucket/warehouse --conf spark.sql.catalog.default_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --conf spark.delta.logStore.class=org.apache.spark.sql.delta.storage.HDFSLogStore --conf spark.sql.parquet.compression.codec=snappy --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,io.delta:delta-core_2.12:2.2.0,org.apache.spark:spark-hadoop-cloud_2.12:3.3.0 --name bronze_sc_stats_etl --verbose --queue root.default --deploy-mode client /opt/***/dags/bronze_ingestion_sc_stats_etl.py --output_path s3a://mybucket/bronze/curry_raw_data/. Error code is: 1.
[2025-06-03T04:23:23.954+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=curry_medallion_etl, task_id=submit_bronze_etl_job, run_id=manual__2025-06-03T04:21:55.534436+00:00, execution_date=20250603T042155, start_date=20250603T042157, end_date=20250603T042323
[2025-06-03T04:23:23.962+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-06-03T04:23:23.962+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 46 for task submit_bronze_etl_job (Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.executor.memory=2g --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minioadmin --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.sql.warehouse.dir=s3a://mybucket/warehouse --conf spark.sql.catalog.default_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --conf spark.delta.logStore.class=org.apache.spark.sql.delta.storage.HDFSLogStore --conf spark.sql.parquet.compression.codec=snappy --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,io.delta:delta-core_2.12:2.2.0,org.apache.spark:spark-hadoop-cloud_2.12:3.3.0 --name bronze_sc_stats_etl --verbose --queue root.default --deploy-mode client /opt/***/dags/bronze_ingestion_sc_stats_etl.py --output_path s3a://mybucket/bronze/curry_raw_data/. Error code is: 1.; 6353)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3005, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 197, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 566, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.executor.memory=2g --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minioadmin --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.sql.warehouse.dir=s3a://mybucket/warehouse --conf spark.sql.catalog.default_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --conf spark.delta.logStore.class=org.apache.spark.sql.delta.storage.HDFSLogStore --conf spark.sql.parquet.compression.codec=snappy --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,io.delta:delta-core_2.12:2.2.0,org.apache.spark:spark-hadoop-cloud_2.12:3.3.0 --name bronze_sc_stats_etl --verbose --queue root.default --deploy-mode client /opt/***/dags/bronze_ingestion_sc_stats_etl.py --output_path s3a://mybucket/bronze/curry_raw_data/. Error code is: 1.
[2025-06-03T04:23:23.986+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-06-03T04:23:24.000+0000] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-06-03T04:23:24.002+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
