FROM apache/airflow:2.10.3-python3.11

ARG AIRFLOW_VERSION=2.10.3

USER root

# Install system dependencies
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        build-essential \
        openjdk-17-jdk \
        python3-dev \
        curl \
        procps \
        wget \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64
ENV PATH="${JAVA_HOME}/bin:${PATH}"

# Set Airflow environment variables for credentials
ENV AIRFLOW_WEBSERVER_AUTH_BACKEND=airflow.providers.apache.hive.auth.backend.default_auth
ENV AIRFLOW_WEBSERVER_AUTHENTICATE=True
ENV AIRFLOW_WEBSERVER_ADMIN_USERNAME=${AIRFLOW_ADMIN_USER}
ENV AIRFLOW_WEBSERVER_ADMIN_PASSWORD=${AIRFLOW_ADMIN_PASSWORD}
ENV AIRFLOW__WEBSERVER__SECRET_KEY="u1UZZLBxgJqpvPSeMTrKZpQxeEzAZmNn_k74FmCmg8="

# Install Spark
ARG SPARK_VERSION=3.5.2
ARG HADOOP_VERSION=3
WORKDIR /opt
RUN wget -q "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && tar -xzf "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && mv "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}" spark \
    && rm "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz"

ENV SPARK_HOME=/opt/spark
ENV PATH="${SPARK_HOME}/bin:${PATH}"

# Add Delta Lake and AWS JARs
ARG DELTA_VERSION=3.2.0
ARG HADOOP_AWS_VERSION=3.3.4
ARG AWS_JAVA_SDK_BUNDLE_VERSION=1.12.262
RUN wget -q "https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/${DELTA_VERSION}/delta-spark_2.12-${DELTA_VERSION}.jar" -P "${SPARK_HOME}/jars/" && \
    wget -q "https://repo1.maven.org/maven2/io/delta/delta-storage/${DELTA_VERSION}/delta-storage-${DELTA_VERSION}.jar" -P "${SPARK_HOME}/jars/" && \
    wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar" -P "${SPARK_HOME}/jars/" && \
    wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar" -P "${SPARK_HOME}/jars/"

USER airflow

# Install Python packages
COPY requirements.txt /tmp/requirements.txt
RUN pip install --no-cache-dir -r /tmp/requirements.txt \
    && pip install --no-cache-dir apache-airflow==${AIRFLOW_VERSION} \
    && pip install --no-cache-dir --upgrade pip

ENV PATH="/home/airflow/.local/bin:${PATH}"
WORKDIR /opt/airflow 