# spark master
spark.master                       spark://spark-master:7077
# hive metastore
spark.hadoop.hive.metastore.uris   thrift://hive-metastore:9083
# delta
spark.sql.extensions                io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog
# minio
spark.hadoop.fs.s3a.endpoint        http://minio:9000
spark.hadoop.fs.s3a.path.style.access true
spark.hadoop.fs.s3a.impl            org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.access.key $MINIO_ACCESS_KEY
spark.hadoop.fs.s3a.secret.key $MINIO_SECRET_KEY

# ======================================================================================
# Performance Tuning Configurations
# ======================================================================================
#
# These settings are configured to maximize resource utilization for faster processing.
#
# -- Resource Allocation ---------------------------------------------------------------
# Use dynamic allocation to let Spark scale executors based on workload.
spark.dynamicAllocation.enabled         true
spark.dynamicAllocation.shuffleTracking.enabled true
spark.dynamicAllocation.minExecutors    1
spark.dynamicAllocation.maxExecutors    10
spark.dynamicAllocation.initialExecutors 2

# -- General Execution -----------------------------------------------------------------
# This tells Spark to use all available cores on the worker nodes for tasks.
spark.cores.max                     16

# -- Memory Settings -------------------------------------------------------------------
# With 3 executors, this will consume ~15GB at peak (3 * (4g + 1g))
spark.driver.memory                 2g
spark.executor.memory               4g
spark.executor.memoryOverhead       1g


